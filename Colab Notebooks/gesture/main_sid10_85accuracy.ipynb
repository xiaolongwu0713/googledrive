{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main_sid10_85accuracy.ipynb","provenance":[{"file_id":"1EYz0RDU00kpyR70LFTuucwD2tc_ry_fQ","timestamp":1618074355729}],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1chE38v0xUsCRQF-8xoGQm2DdkomqgDtd","authorship_tag":"ABX9TyOZc0d3UzPkEMUCxTg0j6oC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CL_MTaBMQ9-p","executionInfo":{"status":"ok","timestamp":1631166919322,"user_tz":-480,"elapsed":547,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}},"outputId":"2c18dd0f-6705-4ed5-b58a-c71924dd8b85"},"source":["%cd /content/drive/MyDrive/\n","# raw_data is imported from global config"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]}]},{"cell_type":"code","metadata":{"id":"xwROBJfQRAYe","executionInfo":{"status":"ok","timestamp":1631166929284,"user_tz":-480,"elapsed":9965,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["%%capture\n","! pip install hdf5storage\n","! pip install mne==0.23.0\n","! pip install torch\n","! pip install tensorflow-gpu == 1.12.0\n","! pip install Braindecode==0.5.1"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9NQVV2AYNaLO"},"source":["***Copy and Paste your code below.***"]},{"cell_type":"code","metadata":{"id":"W2RjuTz5T_ez","executionInfo":{"status":"ok","timestamp":1631166932066,"user_tz":-480,"elapsed":2789,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["import os, re\n","import hdf5storage\n","import numpy as np\n","from scipy.io import savemat\n","from sklearn.model_selection import StratifiedKFold\n","import matplotlib.pyplot as plt\n","from braindecode.datautil import (create_from_mne_raw, create_from_mne_epochs)\n","import torch\n","from braindecode.util import set_random_seeds\n","from skorch.callbacks import LRScheduler\n","from skorch.helper import predefined_split\n","from braindecode import EEGClassifier\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","\n","from braindecode.models import ShallowFBCSPNet,EEGNetv4,Deep4Net\n","from gesture.models.deepmodel import deepnet,deepnet_resnet\n","from gesture.models.EEGModels import DeepConvNet_210519_512_10\n","from gesture.models.tsception import TSception\n","\n","from gesture.myskorch import on_epoch_begin_callback, on_batch_end_callback\n","from gesture.config import *\n","from gesture.preprocess.chn_settings import get_channel_setting\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"q9or620gc2Bk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631166932066,"user_tz":-480,"elapsed":17,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}},"outputId":"b6e9cee0-34d7-4770-a4ee-a2e404ee764b"},"source":["import inspect as i\n","import sys\n","sys.stdout.write(i.getsource(deepnet))"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["class deepnet(nn.Sequential):\n","    \"\"\"\n","    Deep ConvNet model from [1]_.\n","\n","    References\n","    ----------\n","\n","    .. [1] Schirrmeister, R. T., Springenberg, J. T., Fiederer, L. D. J.,\n","       Glasstetter, M., Eggensperger, K., Tangermann, M., Hutter, F. & Ball, T. (2017).\n","       Deep learning with convolutional neural networks for EEG decoding and\n","       visualization.\n","       Human Brain Mapping , Aug. 2017. Online: http://dx.doi.org/10.1002/hbm.23730\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        in_chans,\n","        n_classes,\n","        input_window_samples,\n","        final_conv_length,\n","        n_filters_time=64,\n","        n_filters_spat=64,\n","        filter_time_length=50,\n","        pool_time_length=3,\n","        pool_time_stride=3,\n","        n_filters_2=50,\n","        filter_length_2=10,\n","        n_filters_3=50,\n","        filter_length_3=10,\n","        n_filters_4=50,\n","        filter_length_4=10,\n","        first_nonlin=elu,\n","        first_pool_mode=\"max\",\n","        first_pool_nonlin=identity,\n","        later_nonlin=elu,\n","        later_pool_mode=\"max\",\n","        later_pool_nonlin=identity,\n","        drop_prob=0.5,\n","        double_time_convs=False,\n","        split_first_layer=True,\n","        batch_norm=True,\n","        batch_norm_alpha=0.1,\n","        stride_before_pool=False,\n","    ):\n","        super().__init__()\n","        if final_conv_length == \"auto\":\n","            assert input_window_samples is not None\n","        self.in_chans = in_chans\n","        self.n_classes = n_classes\n","        self.input_window_samples = input_window_samples\n","        self.final_conv_length = final_conv_length\n","        self.n_filters_time = n_filters_time\n","        self.n_filters_spat = n_filters_spat\n","        self.filter_time_length = filter_time_length\n","        self.pool_time_length = pool_time_length\n","        self.pool_time_stride = pool_time_stride\n","        self.n_filters_2 = n_filters_2\n","        self.filter_length_2 = filter_length_2\n","        self.n_filters_3 = n_filters_3\n","        self.filter_length_3 = filter_length_3\n","        self.n_filters_4 = n_filters_4\n","        self.filter_length_4 = filter_length_4\n","        self.first_nonlin = first_nonlin\n","        self.first_pool_mode = first_pool_mode\n","        self.first_pool_nonlin = first_pool_nonlin\n","        self.later_nonlin = later_nonlin\n","        self.later_pool_mode = later_pool_mode\n","        self.later_pool_nonlin = later_pool_nonlin\n","        self.drop_prob = drop_prob\n","        self.double_time_convs = double_time_convs\n","        self.split_first_layer = split_first_layer\n","        self.batch_norm = batch_norm\n","        self.batch_norm_alpha = batch_norm_alpha\n","        self.stride_before_pool = stride_before_pool\n","\n","        if self.stride_before_pool:\n","            conv_stride = self.pool_time_stride\n","            pool_stride = 1\n","        else:\n","            conv_stride = 1\n","            pool_stride = self.pool_time_stride\n","        self.add_module(\"ensuredims\", Ensure4d())\n","        pool_class_dict = dict(max=nn.MaxPool2d, mean=AvgPool2dWithConv)\n","        first_pool_class = pool_class_dict[self.first_pool_mode]\n","        later_pool_class = pool_class_dict[self.later_pool_mode]\n","        if self.split_first_layer:\n","            self.add_module(\"dimshuffle\", Expression(transpose_time_to_spat))\n","            self.add_module(\"conv_time\",nn.Conv2d(1,self.n_filters_time,(self.filter_time_length, 1),stride=1,),)\n","            self.add_module(\"conv_spat\",nn.Conv2d(self.n_filters_time,self.n_filters_spat,(1, self.in_chans),\n","                                                  stride=(conv_stride, 1),bias=not self.batch_norm,),)\n","            n_filters_conv = self.n_filters_spat\n","        else:\n","            self.add_module(\"conv_time\",nn.Conv2d(self.in_chans,self.n_filters_time,(self.filter_time_length, 1),\n","                                                  stride=(conv_stride, 1),bias=not self.batch_norm,),)\n","            n_filters_conv = self.n_filters_time\n","        if self.batch_norm:\n","            self.add_module(\"bnorm\",nn.BatchNorm2d(n_filters_conv,momentum=self.batch_norm_alpha,affine=True,eps=1e-5,),)\n","        self.add_module(\"conv_nonlin\", Expression(self.first_nonlin)) #elu\n","        self.add_module(\"pool\",first_pool_class(kernel_size=(self.pool_time_length, 1), stride=(pool_stride, 1)),) #MaxPool2d\n","        #self.add_module(\"pool_nonlin\", Expression(self.first_pool_nonlin)) # identity\n","\n","        def add_conv_pool_block(model, n_filters_before, n_filters, filter_length, block_nr):\n","            suffix = \"_{:d}\".format(block_nr)\n","            self.add_module(\"drop\" + suffix, nn.Dropout(p=self.drop_prob))\n","            self.add_module(\"conv\" + suffix,nn.Conv2d(n_filters_before,n_filters,(filter_length, 1),\n","                    stride=(conv_stride, 1),bias=not self.batch_norm,),)\n","            if self.batch_norm:\n","                self.add_module(\"bnorm\" + suffix,nn.BatchNorm2d(n_filters,momentum=self.batch_norm_alpha,affine=True,eps=1e-5,),)\n","            self.add_module(\"nonlin\" + suffix, Expression(self.later_nonlin)) # elu\n","\n","            # maxpool2d\n","            #self.add_module(\"pool\" + suffix,later_pool_class(kernel_size=(self.pool_time_length, 1),stride=(pool_stride, 1),),)\n","\n","            #Expression(expression=identity)\n","            #self.add_module(\"pool_nonlin\" + suffix, Expression(self.later_pool_nonlin)) # identity\n","\n","        add_conv_pool_block(self, n_filters_conv, self.n_filters_2, self.filter_length_2, 2)\n","        add_conv_pool_block(self, self.n_filters_2, self.n_filters_3, self.filter_length_3, 3)\n","        add_conv_pool_block(self, self.n_filters_3, self.n_filters_4, self.filter_length_4, 4)\n","\n","        self.add_module(\"last_drop\", nn.Dropout(p=self.drop_prob))\n","\n","        # self.add_module('drop_classifier', nn.Dropout(p=self.drop_prob))\n","        self.eval()\n","        if self.final_conv_length == \"auto\":\n","            out = self(np_to_var(np.ones((1, self.in_chans, self.input_window_samples, 1),dtype=np.float32,)))\n","            n_channels=out.cpu().data.numpy().shape[1]\n","            n_out_time,n_out_spatial = out.cpu().data.numpy().shape[2],out.cpu().data.numpy().shape[3]\n","            self.final_conv_length = n_out_time\n","\n","        #self.add_module(\"conv_classifier\",nn.Conv2d(self.n_filters_4,self.n_classes,(self.final_conv_length, 1),bias=True,),)\n","        self.add_module(\"globalAvgPooling\",nn.AvgPool2d((n_out_time,n_out_spatial)))\n","        self.add_module(\"squeeze1\", Expression(squeeze_all))\n","        self.add_module(\"conv_classifier\", nn.Linear(n_channels,n_classes))\n","        self.add_module(\"softmax\", nn.LogSoftmax(dim=1))\n","        #self.add_module(\"squeeze2\", Expression(squeeze_final_output))\n","\n","        # Initialization, xavier is same as in our paper...\n","        # was default from lasagne\n","        init.xavier_uniform_(self.conv_time.weight, gain=1)\n","        # maybe no bias in case of no split layer and batch norm\n","        if self.split_first_layer or (not self.batch_norm):\n","            init.constant_(self.conv_time.bias, 0)\n","        if self.split_first_layer:\n","            init.xavier_uniform_(self.conv_spat.weight, gain=1)\n","            if not self.batch_norm:\n","                init.constant_(self.conv_spat.bias, 0)\n","        if self.batch_norm:\n","            init.constant_(self.bnorm.weight, 1)\n","            init.constant_(self.bnorm.bias, 0)\n","        param_dict = dict(list(self.named_parameters()))\n","        for block_nr in range(2, 5):\n","            conv_weight = param_dict[\"conv_{:d}.weight\".format(block_nr)]\n","            init.xavier_uniform_(conv_weight, gain=1)\n","            if not self.batch_norm:\n","                conv_bias = param_dict[\"conv_{:d}.bias\".format(block_nr)]\n","                init.constant_(conv_bias, 0)\n","            else:\n","                bnorm_weight = param_dict[\"bnorm_{:d}.weight\".format(block_nr)]\n","                bnorm_bias = param_dict[\"bnorm_{:d}.bias\".format(block_nr)]\n","                init.constant_(bnorm_weight, 1)\n","                init.constant_(bnorm_bias, 0)\n","\n","        init.xavier_uniform_(self.conv_classifier.weight, gain=1)\n","        init.constant_(self.conv_classifier.bias, 0)\n","\n","        # Start in eval mode\n","        self.eval()\n"]}]},{"cell_type":"code","metadata":{"id":"Svy7ABlg3wuE","executionInfo":{"status":"ok","timestamp":1631166932067,"user_tz":-480,"elapsed":14,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["pn=10 #4\n","Session_num,UseChn,EmgChn,TrigChn, activeChn = get_channel_setting(pn)\n","#fs=[Frequencies[i,1] for i in range(Frequencies.shape[0]) if Frequencies[i,0] == pn][0]\n","fs=1000"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_np8cCh3reZN","executionInfo":{"status":"ok","timestamp":1631166932067,"user_tz":-480,"elapsed":13,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}},"outputId":"914bd779-87ba-4e72-fa05-6a2c621f727c"},"source":["[Frequencies[i,1] for i in range(Frequencies.shape[0]) if Frequencies[i,0] == pn][0]"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2000"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"1LBbiAuvUVb_","executionInfo":{"status":"ok","timestamp":1631166948609,"user_tz":-480,"elapsed":16550,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["loadPath = data_dir+'preprocessing'+'/P'+str(pn)+'/preprocessing2.mat'\n","mat=hdf5storage.loadmat(loadPath)\n","data = mat['Datacell']\n","channelNum=int(mat['channelNum'][0,0])\n","data=np.concatenate((data[0,0],data[0,1]),0)\n","del mat\n","# standardization\n","# no effect. why?\n","chn_data=data[:,-3:]\n","data=data[:,:-3]\n","scaler = StandardScaler()\n","scaler.fit(data)\n","data=scaler.transform((data))\n","data=np.concatenate((data,chn_data),axis=1)\n","\n","# stim0 is trigger channel, stim1 is trigger position calculated from EMG signal.\n","chn_names=np.append([\"seeg\"]*len(UseChn),[\"stim0\", \"emg\",\"stim1\"])\n","chn_types=np.append([\"seeg\"]*len(UseChn),[\"stim\", \"emg\",\"stim\"])\n","info = mne.create_info(ch_names=list(chn_names), ch_types=list(chn_types), sfreq=fs)\n","raw = mne.io.RawArray(data.transpose(), info)\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"NxbH3k2Ej3SF","executionInfo":{"status":"ok","timestamp":1631166948613,"user_tz":-480,"elapsed":8,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["# gesture/events type: 1,2,3,4,5\n","events0 = mne.find_events(raw, stim_channel='stim0')\n","events1 = mne.find_events(raw, stim_channel='stim1')\n","# events number should start from 0: 0,1,2,3,4, instead of 1,2,3,4,5\n","events0=events0-[0,0,1]\n","events1=events1-[0,0,1]\n","\n","#print(events[:5])  # show the first 5\n","# Epoch from 4s before(idle) until 4s after(movement) stim1.\n","raw=raw.pick([\"seeg\"])\n","epochs = mne.Epochs(raw, events1, tmin=0, tmax=4,baseline=None)\n","# or epoch from 0s to 4s which only contain movement data.\n","# epochs = mne.Epochs(raw, events1, tmin=0, tmax=4,baseline=None)\n","\n","epoch1=epochs['0'] # 20 trials. 8001 time points per trial for 8s.\n","epoch2=epochs['1']\n","epoch3=epochs['2']\n","epoch4=epochs['3']\n","epoch5=epochs['4']\n","list_of_epochs=[epoch1,epoch2,epoch3,epoch4,epoch5]"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"edBaRd88j-DC","executionInfo":{"status":"ok","timestamp":1631166950953,"user_tz":-480,"elapsed":2346,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["#note: windows_datasets is of class BaseConcatDataset. windows_datasets.datasets is a list of all\n","# trials (like an epoch but organized as a list) epoched from a run.\n","#windows_datasets.datasets[0].windows is an epoch again created by a sliding window from one trial.\n","\n","\n","# 20 trials/epoch * 5 epochs =100 trials=100 datasets\n","# 1 dataset can be slided into ~161(depends on wind_size and stride) windows.\n","windows_datasets = create_from_mne_epochs(\n","    list_of_epochs,\n","    window_size_samples=500,\n","    window_stride_samples=250,\n","    drop_last_window=False\n",")\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"WniFNShFkQwJ","executionInfo":{"status":"ok","timestamp":1631166950957,"user_tz":-480,"elapsed":7,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["# train/valid/test split based on description column\n","desc=windows_datasets.description\n","desc=desc.rename(columns={0: 'split'})\n","trials_per_epoch=epoch1.events.shape[0] # 20 trial per epoch list/class\n","import random\n","val_test_num=2 # two val and two test trials\n","random_index = random.sample(range(trials_per_epoch), val_test_num*2)\n","sorted(random_index)\n","val_index=[rand+iclass*20 for iclass in range(5) for rand in sorted(random_index[:2]) ]\n","test_index=[rand+iclass*20 for iclass in range(5) for rand in sorted(random_index[-2:])]\n","train_index=[item for  item in list(range(100)) if item not in val_index+test_index]\n","desc.iloc[val_index]='validate'\n","desc.iloc[test_index]='test'\n","desc.iloc[train_index]='train'\n","# make sure there are val_test_num trials from each epoch (5 intotal) for both validate and test dataset\n","assert desc[desc['split'] == 'validate'].size == desc[desc['split'] == 'test'].size == val_test_num*5\n","windows_datasets.description=desc\n","splitted = windows_datasets.split('split')\n","\n","train_set = splitted['train']\n","valid_set = splitted['validate']\n","test_set = splitted['test']"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"SIIBAOLXkY2k","executionInfo":{"status":"ok","timestamp":1631166950957,"user_tz":-480,"elapsed":5,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n","device = 'cuda' if cuda else 'cpu'\n","if cuda:\n","    torch.backends.cudnn.benchmark = True\n","seed = 20200220  # random seed to make results reproducible\n","# Set random seed to be able to reproduce results\n","set_random_seeds(seed=seed, cuda=cuda)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"nSrQqFFGkeK-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631166953813,"user_tz":-480,"elapsed":2861,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}},"outputId":"d3a820b8-7046-47d0-c580-baf2f4b757a9"},"source":["n_classes = 5\n","# Extract number of chans and time steps from dataset\n","one_window=windows_datasets.datasets[0].windows.get_data()\n","n_chans = one_window.shape[1]\n","input_window_samples = one_window.shape[2]\n","\n","#model = ShallowFBCSPNet(n_chans,n_classes,input_window_samples=input_window_samples,final_conv_length='auto',) # 51%\n","#model = EEGNetv4(n_chans,n_classes,input_window_samples=input_window_samples,final_conv_length='auto',)\n","#model = deepnet(n_chans,n_classes,input_window_samples=input_window_samples,final_conv_length='auto',)\n","model = deepnet(n_chans,n_classes,input_window_samples=input_window_samples,final_conv_length='auto',) #87%\n","\n","#expand=True/False with 4 blocks: 69%; no block 4(conv_channels=all64):78% ; no block 4(conv_channels=64,50505050):76%\n","#model = deepnet_resnet(n_chans,n_classes,input_window_samples=input_window_samples,expand=False) \n","\n","\n","#model=TSception(1000,n_chans,3,3,0.5)\n","# Send model to GPU\n","if cuda:\n","    model.cuda()\n"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]}]},{"cell_type":"code","metadata":{"id":"bDqnhvuURw59","executionInfo":{"status":"ok","timestamp":1631166953816,"user_tz":-480,"elapsed":15,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":[""],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"icHBaZeXkn85","executionInfo":{"status":"ok","timestamp":1631166953816,"user_tz":-480,"elapsed":15,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["# These values we found good for shallow network:\n","lr = 0.0001\n","weight_decay = 1e-10\n","batch_size = 32\n","n_epochs = 100"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"9cJpa6_TZkLg","executionInfo":{"status":"ok","timestamp":1631166953816,"user_tz":-480,"elapsed":14,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["location=os.getcwd()\n","if re.compile('/Users/long/').match(location):\n","    my_callbacks=[\n","        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n","        ('on_epoch_begin_callback', on_epoch_begin_callback),('on_batch_end_callback',on_batch_end_callback),\n","    ]\n","elif re.compile('/content/drive').match(location):\n","   my_callbacks=[\n","        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n","    ]"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"uf5hmYVAkr9i"},"source":["clf = EEGClassifier(\n","    model,\n","    criterion=torch.nn.NLLLoss,  #torch.nn.NLLLoss/CrossEntropyLoss\n","    optimizer=torch.optim.Adam, #optimizer=torch.optim.AdamW,\n","    train_split=predefined_split(valid_set),  # using valid_set for validation; None means no validate:both train and test on training dataset.\n","    optimizer__lr=lr,\n","    optimizer__weight_decay=weight_decay,\n","    batch_size=batch_size,\n","    callbacks=my_callbacks,\n","    device=device,\n",")\n","# Model training for a specified number of epochs. `y` is None as it is already supplied\n","# in the dataset.\n","clf.fit(train_set, y=None, epochs=n_epochs)"],"execution_count":null,"outputs":[]}]}