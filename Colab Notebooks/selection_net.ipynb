{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"selection_net.ipynb","provenance":[{"file_id":"1EYz0RDU00kpyR70LFTuucwD2tc_ry_fQ","timestamp":1618074355729}],"machine_shape":"hm","mount_file_id":"107-96-H-9eVoAjmrBQe3sMCDxRv6Ig3m","authorship_tag":"ABX9TyNnAHlr71wrJjC9lSxJKdM6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iRCmcYSt9wqh","executionInfo":{"status":"ok","timestamp":1624626545123,"user_tz":-60,"elapsed":198,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}},"outputId":"86b1dffb-cf3c-49dd-ffe1-70ea73f18a4a"},"source":["%cd /content/drive/MyDrive/\n","# raw_data is imported from global config"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pznHHra4_aOg","executionInfo":{"status":"ok","timestamp":1624626551282,"user_tz":-60,"elapsed":5872,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["%%capture\n","! pip install mne==0.19.2;\n","! pip install torch==1.7.0;"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"xJxokHgH95AS","executionInfo":{"status":"ok","timestamp":1624626553064,"user_tz":-60,"elapsed":1785,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["import torch\n","import numpy as np\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","\n","from torch.utils.data import DataLoader\n","from grasp.TSception.utils import regulization\n","from grasp.utils import SEEGDataset, load_data, SEEGDataset3D, cuda_or_cup, set_random_seeds\n","from grasp.TSception.Models import TSception2,wholenet\n","from grasp.braindecode.Models import shallowConv,deepConv\n","from grasp.process.channel_settings import badtrials\n","from grasp.config import root_dir"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"NyEOXTXgHlc4","executionInfo":{"status":"ok","timestamp":1624626553068,"user_tz":-60,"elapsed":10,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["import sys, importlib\n","importlib.reload(sys.modules['grasp.TSception.Models'])\n","from grasp.TSception.Models import TSception2,wholenet,SelectionLayer"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"fHeYjqDCJ57K","executionInfo":{"status":"ok","timestamp":1624626553069,"user_tz":-60,"elapsed":9,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["import inspect as i\n","import sys\n","#sys.stdout.write(i.getsource(SelectionLayer))"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UgdufZcJ960w","executionInfo":{"status":"ok","timestamp":1624626553069,"user_tz":-60,"elapsed":9,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}},"outputId":"4dec4556-217d-4a65-db12-e7bf8b9e7f10"},"source":["device=cuda_or_cup()\n","enable_cuda = torch.cuda.is_available()\n","print('GPU computing: ', enable_cuda)\n","seed = 123456789  # random seed to make results reproducible\n","# Set random seed to be able to reproduce results\n","set_random_seeds(seed=seed)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["GPU computing:   True\n","GPU computing:  True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h3nGSPVo-F1W","executionInfo":{"status":"ok","timestamp":1624626553070,"user_tz":-60,"elapsed":8,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["sid=10 # 10 converge faster\n","optins=['normalized_frequency_and_raw','frequency_and_raw','raw']\n","input=optins[2]\n","\n","#result_dir=root_dir+'grasp/TSception/shallowConv'+str(sid)+'/'\n","result_dir=root_dir+'grasp/TSception/result_TSSmall'+'_'+input+str(sid)+'/'\n","saved_numpy=result_dir+'result/'\n","pths=result_dir+'pth/'\n","pro_dir=result_dir + '/probs/'\n","\n","import os\n","if not os.path.exists(result_dir):\n","    os.makedirs(result_dir)\n","if not os.path.exists(saved_numpy):\n","    os.makedirs(saved_numpy)\n","if not os.path.exists(pths):\n","    os.makedirs(pths)\n","if not os.path.exists(pro_dir):\n","    os.makedirs(pro_dir)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"5g2PPSlO-IxJ","executionInfo":{"status":"ok","timestamp":1624626564048,"user_tz":-60,"elapsed":10985,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["%%capture\n","# suppress the output\n","traindata, valdata, testdata = load_data(sid,split=True,move2=True,input=input)\n","traindata = traindata.transpose(2, 0, 1)  #-->(trials94,channels,  time)\n","valdata = valdata.transpose(2, 0, 1) # 32\n","testdata = testdata.transpose(2, 0, 1)  # 8"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"P59dQr40-LQA","executionInfo":{"status":"ok","timestamp":1624626564051,"user_tz":-60,"elapsed":16,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["# Total trial number from train, val and test dataset should be equal to total trial from config file.\n","total_trials1=traindata.shape[0]+valdata.shape[0]+testdata.shape[0]\n","total_trials2=4*40-(len(badtrials[sid][0])+len(badtrials[sid][1])+len(badtrials[sid][2])+len(badtrials[sid][3]))\n","if total_trials1!=total_trials2:\n","    raise SystemExit(\"Trial number dones't match\")\n","trainx, trainy = traindata[:, :-2, :], traindata[:, -2, :] #-2 is real force, -1 is target\n","valx, valy = valdata[:, :-2, :], valdata[:, -2, :]\n","testx, testy = testdata[:, :-2, :], testdata[:, -2, :]"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"kuLv2Qo4-Nap","executionInfo":{"status":"ok","timestamp":1624626564052,"user_tz":-60,"elapsed":16,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["if input=='normalized_frequency_and_raw':\n","    fs=250\n","    step=125\n","    T=250\n","else:\n","    fs=1000\n","    step=500 #ms\n","    T=1000 #ms\n","dataset_train = SEEGDataset3D(trainx, trainy,T,step)\n","dataset_val = SEEGDataset3D(valx, valy,T,step)\n","dataset_test = SEEGDataset3D(testx, testy,T,step)\n","train_loader = DataLoader(dataset=dataset_train, batch_size=1, shuffle=True, pin_memory=False)\n","val_loader = DataLoader(dataset=dataset_val, batch_size=1, pin_memory=False)\n","test_loader = DataLoader(dataset=dataset_test, batch_size=1, pin_memory=False)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"EtWoPea--Ulj","executionInfo":{"status":"ok","timestamp":1624626564052,"user_tz":-60,"elapsed":16,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["epochs=800\n","\n","# TSception parameter\n","sampling_rate=1000\n","learning_rate=0.002\n","totalLen=trainx.shape[2] #ms\n","batch_size=int((totalLen-T)/step) # 280\n","num_T = 3 # (6 conv2d layers) * ( 3 kernel each layer)\n","num_S = 3\n","hidden_size=222\n","dropout=0.5\n","#Lambda = 1e-10\n","\n","# braindecode parameter\n","checkshape=torch.squeeze(next(iter(test_loader))[0],dim=0) # torch.Size([28,1,102,1000])\n","length=checkshape.shape[3] # torch.Size([28, 102, 1000])\n","convfeature=40\n","tkernelSize=200\n","avgpoolKernel=100\n","maxpoolKernel=3\n","maxpoolStride=3\n","blockKernelSize=10\n","input_dim=checkshape.shape[2]\n","\n","# selection neuron\n","M=10"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"h8SqR8xA-hcY","executionInfo":{"status":"ok","timestamp":1624626564052,"user_tz":-60,"elapsed":15,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["#Create schedule for temperature and regularization threshold\n","#Create a vector of length epochs, decaying start_value to end_value exponentially, reaching end_value at end_epoch\n","\n","start_temp=20\n","end_temp=0.1\n","start_thresh=8.0\n","end_thresh=1.1\n","\n","def exponential_decay_schedule(start_value,end_value,epochs,end_epoch):\n","    t = torch.FloatTensor(torch.arange(0.0,epochs))\n","    p = torch.clamp(t/end_epoch,0,1)\n","    out = start_value*torch.pow(end_value/start_value,p)\n","    return out\n","\n","temperature_schedule = exponential_decay_schedule(start_temp,end_temp,epochs,int(epochs*3/4)) #one-hot degree\n","thresh_schedule = exponential_decay_schedule(start_thresh,end_thresh,epochs,epochs) # duplication penalty\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ce1cjQW_-er4","executionInfo":{"status":"ok","timestamp":1624626569630,"user_tz":-60,"elapsed":5593,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["#Network loss function\n","def loss_function(output,target,model,lamba,weight_decay):\n","    l = nn.MSELoss()\n","    sup_loss = l(output,target)\n","    reg = model.regularizer(lamba,weight_decay)\n","    return sup_loss,reg\n","\n","weight_decay=1e-6 #1e-6 # model parameter weight decay\n","lamba=0.1 # 0.1 regularization of penalization of duplication selection\n","\n","enable_select=False\n","# def __init__(self, input_dim, M ,sampling_rate, chnNum, num_T, num_S,dropout):\n","#Question: forward called during initialization throw error: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n","#Solution1: find a PC with GPU to debug.\n","#Solution2: no need to call selection layer forward. Dimmension is determinant for selection layer.\n","shape=checkshape.shape\n","if enable_select==True:\n","    net = wholenet(shape,enable_select,input_dim, M, sampling_rate,M, num_T, num_S,dropout)\n","    net.enable_select = True\n","    net.set_freeze(False)\n","else:\n","    net = wholenet(shape,enable_select,input_dim, M, sampling_rate, input_dim, num_T, num_S, dropout)\n","    net.enable_select = False\n","\n","\n","if(enable_cuda):\n","    net.cuda()\n","optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n","#optimizer = torch.optim.Adagrad(net.parameters(), lr=learning_rate,weight_decay=1e-4)\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"In64_47aLH-U","executionInfo":{"status":"ok","timestamp":1624626569635,"user_tz":-60,"elapsed":19,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}},"outputId":"c078487a-7014-4fe7-d843-64e4e4aecf78"},"source":["torch.cuda.is_available()"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"XoDEwAEB-fTQ","executionInfo":{"status":"ok","timestamp":1624626569635,"user_tz":-60,"elapsed":7,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["#checkpoint = torch.load(result_dir+'checkpoint440.pth')\n","#net.load_state_dict(checkpoint['net'])\n","#optimizer.load_state_dict(checkpoint['optimizer'])"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"id":"q0HifOa5-7Lo","executionInfo":{"status":"ok","timestamp":1624626569636,"user_tz":-60,"elapsed":8,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}},"outputId":"1a4abdd1-923d-48df-dc24-99373a2991cd"},"source":["fig2, ax2=plt.subplots()"],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANT0lEQVR4nO3cYYjkd33H8ffHO1NpjKb0VpC706T00njYQtIlTRFqirZc8uDugUXuIFgleGAbKVWEFEuU+MiGWhCu1ZOKVdAYfSALntwDjQTEC7chNXgXItvTeheFrDHNk6Ax7bcPZtKdrneZf3Zndy/7fb/gYP7/+e3Mlx97752d2ZlUFZKk7e8VWz2AJGlzGHxJasLgS1ITBl+SmjD4ktSEwZekJqYGP8lnkzyZ5PuXuD5JPplkKcmjSW6c/ZiSpPUa8gj/c8CBF7n+VmDf+N9R4F/WP5YkadamBr+qHgR+/iJLDgGfr5FTwNVJXj+rASVJs7FzBrexGzg/cXxhfO6nqxcmOcrotwCuvPLKP7z++utncPeS1MfDDz/8s6qaW8vXziL4g1XVceA4wPz8fC0uLm7m3UvSy16S/1zr187ir3SeAPZOHO8Zn5MkXUZmEfwF4F3jv9a5GXimqn7t6RxJ0taa+pROki8BtwC7klwAPgK8EqCqPgWcAG4DloBngfds1LCSpLWbGvyqOjLl+gL+emYTSZI2hO+0laQmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqYlBwU9yIMnjSZaS3HWR69+Q5IEkjyR5NMltsx9VkrQeU4OfZAdwDLgV2A8cSbJ/1bK/B+6vqhuAw8A/z3pQSdL6DHmEfxOwVFXnquo54D7g0Ko1BbxmfPm1wE9mN6IkaRaGBH83cH7i+ML43KSPArcnuQCcAN5/sRtKcjTJYpLF5eXlNYwrSVqrWb1oewT4XFXtAW4DvpDk1267qo5X1XxVzc/Nzc3oriVJQwwJ/hPA3onjPeNzk+4A7geoqu8CrwJ2zWJASdJsDAn+aWBfkmuTXMHoRdmFVWt+DLwNIMmbGAXf52wk6TIyNfhV9TxwJ3ASeIzRX+OcSXJPkoPjZR8E3pvke8CXgHdXVW3U0JKkl27nkEVVdYLRi7GT5+6euHwWeMtsR5MkzZLvtJWkJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNTEo+EkOJHk8yVKSuy6x5p1JziY5k+SLsx1TkrReO6ctSLIDOAb8GXABOJ1koarOTqzZB/wd8JaqejrJ6zZqYEnS2gx5hH8TsFRV56rqOeA+4NCqNe8FjlXV0wBV9eRsx5QkrdeQ4O8Gzk8cXxifm3QdcF2S7yQ5leTAxW4oydEki0kWl5eX1zaxJGlNZvWi7U5gH3ALcAT4TJKrVy+qquNVNV9V83NzczO6a0nSEEOC/wSwd+J4z/jcpAvAQlX9qqp+CPyA0Q8ASdJlYkjwTwP7klyb5ArgMLCwas3XGD26J8kuRk/xnJvhnJKkdZoa/Kp6HrgTOAk8BtxfVWeS3JPk4HjZSeCpJGeBB4APVdVTGzW0JOmlS1VtyR3Pz8/X4uLilty3JL1cJXm4qubX8rW+01aSmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmBgU/yYEkjydZSnLXi6x7R5JKMj+7ESVJszA1+El2AMeAW4H9wJEk+y+y7irgb4CHZj2kJGn9hjzCvwlYqqpzVfUccB9w6CLrPgZ8HPjFDOeTJM3IkODvBs5PHF8Yn/s/SW4E9lbV11/shpIcTbKYZHF5efklDytJWrt1v2ib5BXAJ4APTltbVcerar6q5ufm5tZ715Kkl2BI8J8A9k4c7xmfe8FVwJuBbyf5EXAzsOALt5J0eRkS/NPAviTXJrkCOAwsvHBlVT1TVbuq6pqqugY4BRysqsUNmViStCZTg19VzwN3AieBx4D7q+pMknuSHNzoASVJs7FzyKKqOgGcWHXu7kusvWX9Y0mSZs132kpSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmhgU/CQHkjyeZCnJXRe5/gNJziZ5NMk3k7xx9qNKktZjavCT7ACOAbcC+4EjSfavWvYIMF9VfwB8FfiHWQ8qSVqfIY/wbwKWqupcVT0H3AccmlxQVQ9U1bPjw1PAntmOKUlaryHB3w2cnzi+MD53KXcA37jYFUmOJllMsri8vDx8SknSus30RdsktwPzwL0Xu76qjlfVfFXNz83NzfKuJUlT7Byw5glg78TxnvG5/yfJ24EPA2+tql/OZjxJ0qwMeYR/GtiX5NokVwCHgYXJBUluAD4NHKyqJ2c/piRpvaYGv6qeB+4ETgKPAfdX1Zkk9yQ5OF52L/Bq4CtJ/j3JwiVuTpK0RYY8pUNVnQBOrDp398Tlt894LknSjPlOW0lqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpoYFPwkB5I8nmQpyV0Xuf43knx5fP1DSa6Z9aCSpPWZGvwkO4BjwK3AfuBIkv2rlt0BPF1Vvwv8E/DxWQ8qSVqfIY/wbwKWqupcVT0H3AccWrXmEPBv48tfBd6WJLMbU5K0XjsHrNkNnJ84vgD80aXWVNXzSZ4Bfhv42eSiJEeBo+PDXyb5/lqG3oZ2sWqvGnMvVrgXK9yLFb+31i8cEvyZqarjwHGAJItVNb+Z93+5ci9WuBcr3IsV7sWKJItr/dohT+k8AeydON4zPnfRNUl2Aq8FnlrrUJKk2RsS/NPAviTXJrkCOAwsrFqzAPzl+PJfAN+qqprdmJKk9Zr6lM74Ofk7gZPADuCzVXUmyT3AYlUtAP8KfCHJEvBzRj8Upjm+jrm3G/dihXuxwr1Y4V6sWPNexAfiktSD77SVpCYMviQ1seHB92MZVgzYiw8kOZvk0STfTPLGrZhzM0zbi4l170hSSbbtn+QN2Ysk7xx/b5xJ8sXNnnGzDPg/8oYkDyR5ZPz/5LatmHOjJflskicv9V6ljHxyvE+PJrlx0A1X1Yb9Y/Qi738AvwNcAXwP2L9qzV8BnxpfPgx8eSNn2qp/A/fiT4HfHF9+X+e9GK+7CngQOAXMb/XcW/h9sQ94BPit8fHrtnruLdyL48D7xpf3Az/a6rk3aC/+BLgR+P4lrr8N+AYQ4GbgoSG3u9GP8P1YhhVT96KqHqiqZ8eHpxi952E7GvJ9AfAxRp/L9IvNHG6TDdmL9wLHquppgKp6cpNn3CxD9qKA14wvvxb4ySbOt2mq6kFGf/F4KYeAz9fIKeDqJK+fdrsbHfyLfSzD7kutqarngRc+lmG7GbIXk+5g9BN8O5q6F+NfUfdW1dc3c7AtMOT74jrguiTfSXIqyYFNm25zDdmLjwK3J7kAnADevzmjXXZeak+ATf5oBQ2T5HZgHnjrVs+yFZK8AvgE8O4tHuVysZPR0zq3MPqt78Ekv19V/7WlU22NI8Dnquofk/wxo/f/vLmq/merB3s52OhH+H4sw4ohe0GStwMfBg5W1S83abbNNm0vrgLeDHw7yY8YPUe5sE1fuB3yfXEBWKiqX1XVD4EfMPoBsN0M2Ys7gPsBquq7wKsYfbBaN4N6stpGB9+PZVgxdS+S3AB8mlHst+vztDBlL6rqmaraVVXXVNU1jF7POFhVa/7QqMvYkP8jX2P06J4kuxg9xXNuM4fcJEP24sfA2wCSvIlR8Jc3dcrLwwLwrvFf69wMPFNVP532RRv6lE5t3McyvOwM3It7gVcDXxm/bv3jqjq4ZUNvkIF70cLAvTgJ/HmSs8B/Ax+qqm33W/DAvfgg8Jkkf8voBdx3b8cHiEm+xOiH/K7x6xUfAV4JUFWfYvT6xW3AEvAs8J5Bt7sN90qSdBG+01aSmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElq4n8BzPZcum6w2goAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s6W0vjF1--Iv","outputId":"734fc4c5-4177-43fe-ada2-1a71c6504d18"},"source":["debugg = False\n","#debugg=True\n","for epoch in range(epochs):\n","    print(\"------ epoch \" + str(epoch) + \" -----\")\n","    net.train()\n","    if enable_select==True:\n","        net.set_thresh(thresh_schedule[epoch])\n","        net.set_temperature(temperature_schedule[epoch])\n","\n","    loss_epoch = 0\n","    # trial=0\n","    for trial, (trainx, trainy) in enumerate(train_loader):  # ([1, 15000, 19]), ([1, 15000])\n","        if debugg == True:  # just test one trial\n","            if trial == 1:\n","                break\n","                pass\n","        optimizer.zero_grad()\n","\n","        if (enable_cuda):\n","            x = trainx.float().cuda()\n","            target = trainy.float().cuda()\n","        else:\n","            x = trainx.float()\n","            target = trainy.float()\n","        y_pred = net(x)\n","        # target = torch.from_numpy(target)\n","\n","        sup_loss, reg = loss_function(y_pred, target.float(), net,lamba, weight_decay)\n","        loss = sup_loss + reg  # regulization\n","        loss_epoch+=sup_loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(\"\" + str(epoch) + \" loss:\" + str(loss_epoch / (trial + 1)) + \".\")\n","    if epoch % 2 == 0:\n","        if net.enable_select==True:\n","            H, sel, probas = net.monitor()\n","            ax2.imshow(probas.cpu().detach().numpy(),origin='lower',cmap='RdBu_r',aspect='auto')\n","            im=ax2.images[-1]\n","            fig2.colorbar(im)\n","            fig2.savefig(pro_dir + 'prob_dist' + str(epoch) + '.png')\n","            im.colorbar.remove()\n","            ax2.clear()\n","\n","        net.eval()\n","        print(\"Validating...\")\n","        with torch.no_grad():\n","            vpredAll = []\n","            vtargetAll = []\n","            for trial, (vx, vtarget) in enumerate(val_loader):  # ([1, 15000, 19]), ([1, 15000])\n","                if (enable_cuda):\n","                    vx = vx.float().cuda()\n","                    vtarget = vtarget.float().cuda()\n","                else:\n","                    vx = vx.float()\n","                    vtarget = vtarget.float()\n","                y_pred = net(vx)\n","                \n","\n","                y_pred = y_pred.squeeze().cpu().detach().numpy()\n","                vtarget = vtarget.squeeze().cpu().numpy()\n","                vpredAll.append(y_pred)\n","                vtargetAll.append(vtarget)\n","\n","        vpredAll = np.concatenate(vpredAll, axis=0)\n","        vtargetAll = np.concatenate(vtargetAll, axis=0)\n","        #loss_val = criterion(torch.from_numpy(vpredAll.squeeze()), torch.from_numpy(vtargetAll.squeeze()))\n","\n","        pred_target=np.concatenate((vpredAll[:,None],vtargetAll[:,None]),axis=1)\n","        save_pred=saved_numpy + 'prediction_epoch' + str(epoch) + '.npy'\n","        np.save(save_pred, pred_target)\n","\n","        fig, ax = plt.subplots(figsize=(6, 3))\n","        plt.ion()\n","        ax.clear()\n","        ax.plot(vtargetAll, label=\"True\", linewidth=1)\n","        ax.plot(vpredAll, label='Predicted - Test', linewidth=1)\n","        ax.legend(loc='upper left')\n","        figname = result_dir + 'prediction' + str(epoch) + '.png'\n","        fig.savefig(figname)\n","        plt.close(fig)\n","    if epoch % 10 == 0:\n","        state = {\n","            'net': net.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","        }\n","        savepath = pths + 'checkpoint' + str(epoch) + '.pth'\n","        torch.save(state, savepath)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["------ epoch 0 -----\n","0 loss:0.3050936886833774.\n","Validating...\n","------ epoch 1 -----\n","1 loss:0.23170387977336207.\n","------ epoch 2 -----\n","2 loss:0.2128657030944641.\n","Validating...\n","------ epoch 3 -----\n","3 loss:0.20627449319148675.\n","------ epoch 4 -----\n","4 loss:0.20107152545426646.\n","Validating...\n","------ epoch 5 -----\n","5 loss:0.19308316858851501.\n","------ epoch 6 -----\n","6 loss:0.18378074783991036.\n","Validating...\n","------ epoch 7 -----\n","7 loss:0.16785681820036763.\n","------ epoch 8 -----\n","8 loss:0.16834854337776828.\n","Validating...\n","------ epoch 9 -----\n","9 loss:0.16120502475298878.\n","------ epoch 10 -----\n","10 loss:0.15912166873040873.\n","Validating...\n","------ epoch 11 -----\n","11 loss:0.16786988656649476.\n","------ epoch 12 -----\n","12 loss:0.16548707383947495.\n","Validating...\n","------ epoch 13 -----\n","13 loss:0.16085343190238008.\n","------ epoch 14 -----\n","14 loss:0.15915971172925752.\n","Validating...\n","------ epoch 15 -----\n","15 loss:0.15122573951689097.\n","------ epoch 16 -----\n","16 loss:0.1431229416694906.\n","Validating...\n","------ epoch 17 -----\n","17 loss:0.1378924873076443.\n","------ epoch 18 -----\n","18 loss:0.1334401931709204.\n","Validating...\n","------ epoch 19 -----\n","19 loss:0.12826467617454693.\n","------ epoch 20 -----\n","20 loss:0.12823406470000234.\n","Validating...\n","------ epoch 21 -----\n","21 loss:0.12856139214789.\n","------ epoch 22 -----\n","22 loss:0.1297096728036801.\n","Validating...\n","------ epoch 23 -----\n","23 loss:0.13611880481306815.\n","------ epoch 24 -----\n","24 loss:0.12933313295754612.\n","Validating...\n","------ epoch 25 -----\n","25 loss:0.13378221021057704.\n","------ epoch 26 -----\n","26 loss:0.1431602966836375.\n","Validating...\n","------ epoch 27 -----\n","27 loss:0.13658878379143202.\n","------ epoch 28 -----\n","28 loss:0.1363006922392509.\n","Validating...\n","------ epoch 29 -----\n","29 loss:0.12375633729000886.\n","------ epoch 30 -----\n","30 loss:0.12334449278811614.\n","Validating...\n","------ epoch 31 -----\n","31 loss:0.1486482130857105.\n","------ epoch 32 -----\n","32 loss:0.14358014791694462.\n","Validating...\n","------ epoch 33 -----\n","33 loss:0.139897687965606.\n","------ epoch 34 -----\n","34 loss:0.14410767761560586.\n","Validating...\n","------ epoch 35 -----\n","35 loss:0.13620044646036422.\n","------ epoch 36 -----\n","36 loss:0.13046022234723353.\n","Validating...\n","------ epoch 37 -----\n","37 loss:0.1321147448486752.\n","------ epoch 38 -----\n","38 loss:0.13413639415978876.\n","Validating...\n","------ epoch 39 -----\n","39 loss:0.13195403058750507.\n","------ epoch 40 -----\n","40 loss:0.12700208980176184.\n","Validating...\n","------ epoch 41 -----\n","41 loss:0.12253953885828328.\n","------ epoch 42 -----\n","42 loss:0.12890980526422843.\n","Validating...\n","------ epoch 43 -----\n","43 loss:0.13313911075138637.\n","------ epoch 44 -----\n","44 loss:0.12353787215378804.\n","Validating...\n","------ epoch 45 -----\n","45 loss:0.12202374855231525.\n","------ epoch 46 -----\n","46 loss:0.1286160001833724.\n","Validating...\n","------ epoch 47 -----\n","47 loss:0.12745969737760532.\n","------ epoch 48 -----\n","48 loss:0.12304118003409642.\n","Validating...\n","------ epoch 49 -----\n","49 loss:0.14604111526034072.\n","------ epoch 50 -----\n","50 loss:0.12292524474935654.\n","Validating...\n","------ epoch 51 -----\n","51 loss:0.19624573552710378.\n","------ epoch 52 -----\n","52 loss:0.1678482524286478.\n","Validating...\n","------ epoch 53 -----\n","53 loss:0.15625660630882296.\n","------ epoch 54 -----\n","54 loss:0.13091002880699104.\n","Validating...\n","------ epoch 55 -----\n","55 loss:0.12728488470754054.\n","------ epoch 56 -----\n","56 loss:0.12530205856499255.\n","Validating...\n","------ epoch 57 -----\n","57 loss:0.12173465728505045.\n","------ epoch 58 -----\n","58 loss:0.12542004945377508.\n","Validating...\n","------ epoch 59 -----\n","59 loss:0.1215862729626461.\n","------ epoch 60 -----\n","60 loss:0.11852181274603066.\n","Validating...\n","------ epoch 61 -----\n","61 loss:0.11645677118984044.\n","------ epoch 62 -----\n","62 loss:0.11459428729473525.\n","Validating...\n","------ epoch 63 -----\n","63 loss:0.11272796064328688.\n","------ epoch 64 -----\n","64 loss:0.11064766435366538.\n","Validating...\n","------ epoch 65 -----\n","65 loss:0.1114072553638337.\n","------ epoch 66 -----\n","66 loss:0.10918461112703523.\n","Validating...\n","------ epoch 67 -----\n","67 loss:0.1135559025594694.\n","------ epoch 68 -----\n","68 loss:0.11428845105453944.\n","Validating...\n","------ epoch 69 -----\n","69 loss:0.10603361888032438.\n","------ epoch 70 -----\n","70 loss:0.10516336612785473.\n","Validating...\n","------ epoch 71 -----\n","71 loss:0.10944044992773451.\n","------ epoch 72 -----\n","72 loss:0.11056162365194824.\n","Validating...\n","------ epoch 73 -----\n","73 loss:0.10621017574245094.\n","------ epoch 74 -----\n","74 loss:0.11222075228380342.\n","Validating...\n","------ epoch 75 -----\n","75 loss:0.11772475980667986.\n","------ epoch 76 -----\n","76 loss:0.10554169152632482.\n","Validating...\n","------ epoch 77 -----\n","77 loss:0.1073870605462764.\n","------ epoch 78 -----\n","78 loss:0.10102978609422715.\n","Validating...\n","------ epoch 79 -----\n","79 loss:0.11236820246578537.\n","------ epoch 80 -----\n","80 loss:0.11201348023600557.\n","Validating...\n","------ epoch 81 -----\n","81 loss:0.10776780434271209.\n","------ epoch 82 -----\n","82 loss:0.10468727733112043.\n","Validating...\n","------ epoch 83 -----\n","83 loss:0.11246715049044444.\n","------ epoch 84 -----\n","84 loss:0.10990007716016127.\n","Validating...\n","------ epoch 85 -----\n","85 loss:0.11382483790477371.\n","------ epoch 86 -----\n","86 loss:0.13533043073346981.\n","Validating...\n","------ epoch 87 -----\n","87 loss:0.11753600799184069.\n","------ epoch 88 -----\n","88 loss:0.11118943584907769.\n","Validating...\n","------ epoch 89 -----\n","89 loss:0.11067898251100355.\n","------ epoch 90 -----\n","90 loss:0.1047246708876939.\n","Validating...\n","------ epoch 91 -----\n","91 loss:0.10858049874918328.\n","------ epoch 92 -----\n","92 loss:0.10495996587456037.\n","Validating...\n","------ epoch 93 -----\n","93 loss:0.10753284104200256.\n","------ epoch 94 -----\n","94 loss:0.10599704205384876.\n","Validating...\n","------ epoch 95 -----\n","95 loss:0.12332745243866856.\n","------ epoch 96 -----\n","96 loss:0.10841196749964331.\n","Validating...\n","------ epoch 97 -----\n","97 loss:0.10153933942445323.\n","------ epoch 98 -----\n","98 loss:0.1057520931841344.\n","Validating...\n","------ epoch 99 -----\n","99 loss:0.10190463833447196.\n","------ epoch 100 -----\n","100 loss:0.10156082353976548.\n","Validating...\n","------ epoch 101 -----\n","101 loss:0.11372569841770534.\n","------ epoch 102 -----\n","102 loss:0.1718445682627523.\n","Validating...\n","------ epoch 103 -----\n","103 loss:0.14399722632434633.\n","------ epoch 104 -----\n","104 loss:0.1260472904636055.\n","Validating...\n","------ epoch 105 -----\n","105 loss:0.11291416428792171.\n","------ epoch 106 -----\n","106 loss:0.11665199302200578.\n","Validating...\n","------ epoch 107 -----\n","107 loss:0.10834392856679156.\n","------ epoch 108 -----\n","108 loss:0.1061948438724264.\n","Validating...\n","------ epoch 109 -----\n","109 loss:0.11076235689986975.\n","------ epoch 110 -----\n","110 loss:0.13142177766650662.\n","Validating...\n","------ epoch 111 -----\n","111 loss:0.10952067581347676.\n","------ epoch 112 -----\n","112 loss:0.10562767124233338.\n","Validating...\n","------ epoch 113 -----\n","113 loss:0.12716152485555562.\n","------ epoch 114 -----\n","114 loss:0.1239369039177003.\n","Validating...\n","------ epoch 115 -----\n","115 loss:0.1222747264143366.\n","------ epoch 116 -----\n","116 loss:0.11156854247594745.\n","Validating...\n","------ epoch 117 -----\n","117 loss:0.10529472592931527.\n","------ epoch 118 -----\n","118 loss:0.10972451659826896.\n","Validating...\n","------ epoch 119 -----\n","119 loss:0.1286712530681975.\n","------ epoch 120 -----\n","120 loss:0.13046053318609285.\n","Validating...\n","------ epoch 121 -----\n","121 loss:0.1198108497624978.\n","------ epoch 122 -----\n","122 loss:0.11005224737251161.\n","Validating...\n","------ epoch 123 -----\n","123 loss:0.1155623235206446.\n","------ epoch 124 -----\n","124 loss:0.1093897249820268.\n","Validating...\n","------ epoch 125 -----\n","125 loss:0.1236253346547357.\n","------ epoch 126 -----\n","126 loss:0.14765379317184416.\n","Validating...\n","------ epoch 127 -----\n","127 loss:0.13822894342020792.\n","------ epoch 128 -----\n","128 loss:0.1363292929326367.\n","Validating...\n","------ epoch 129 -----\n","129 loss:0.13133286910816136.\n","------ epoch 130 -----\n","130 loss:0.13147981598591194.\n","Validating...\n","------ epoch 131 -----\n","131 loss:0.12602740567591456.\n","------ epoch 132 -----\n","132 loss:0.12420925640493122.\n","Validating...\n","------ epoch 133 -----\n","133 loss:0.12797965811422238.\n","------ epoch 134 -----\n","134 loss:0.12577497862024695.\n","Validating...\n","------ epoch 135 -----\n","135 loss:0.12044839510041425.\n","------ epoch 136 -----\n","136 loss:0.11960043570297396.\n","Validating...\n","------ epoch 137 -----\n","137 loss:0.11969317083493766.\n","------ epoch 138 -----\n","138 loss:0.14370798122169626.\n","Validating...\n","------ epoch 139 -----\n","139 loss:0.13817406544445926.\n","------ epoch 140 -----\n","140 loss:0.13867947769661745.\n","Validating...\n","------ epoch 141 -----\n","141 loss:0.13603515507510075.\n","------ epoch 142 -----\n","142 loss:0.12993199733269012.\n","Validating...\n","------ epoch 143 -----\n","143 loss:0.1373062809674531.\n","------ epoch 144 -----\n","144 loss:0.13882014241356117.\n","Validating...\n","------ epoch 145 -----\n","145 loss:0.14317376519211084.\n","------ epoch 146 -----\n","146 loss:0.13910596962604257.\n","Validating...\n","------ epoch 147 -----\n","147 loss:0.13664005558269146.\n","------ epoch 148 -----\n","148 loss:0.1364908382280642.\n","Validating...\n","------ epoch 149 -----\n","149 loss:0.13218086995343623.\n","------ epoch 150 -----\n","150 loss:0.12688026320125556.\n","Validating...\n","------ epoch 151 -----\n","151 loss:0.12366956649905342.\n","------ epoch 152 -----\n","152 loss:0.12393861468562968.\n","Validating...\n","------ epoch 153 -----\n","153 loss:0.12401616691142066.\n","------ epoch 154 -----\n","154 loss:0.11817311727179167.\n","Validating...\n","------ epoch 155 -----\n","155 loss:0.11737784768781091.\n","------ epoch 156 -----\n","156 loss:0.116785237334796.\n","Validating...\n","------ epoch 157 -----\n","157 loss:0.11799765420424888.\n","------ epoch 158 -----\n","158 loss:0.11592408758389135.\n","Validating...\n","------ epoch 159 -----\n","159 loss:0.11475842883093999.\n","------ epoch 160 -----\n","160 loss:0.11499037437586702.\n","Validating...\n","------ epoch 161 -----\n","161 loss:0.10871979335927938.\n","------ epoch 162 -----\n","162 loss:0.10481506442794433.\n","Validating...\n","------ epoch 163 -----\n","163 loss:0.1041485896636533.\n","------ epoch 164 -----\n","164 loss:0.1043997674018272.\n","Validating...\n","------ epoch 165 -----\n","165 loss:0.1011730841973908.\n","------ epoch 166 -----\n","166 loss:0.09882309491165046.\n","Validating...\n","------ epoch 167 -----\n","167 loss:0.10097394896368696.\n","------ epoch 168 -----\n","168 loss:0.10062814073668969.\n","Validating...\n","------ epoch 169 -----\n","169 loss:0.11635290214027731.\n","------ epoch 170 -----\n","170 loss:0.11146683569074187.\n","Validating...\n","------ epoch 171 -----\n","171 loss:0.10774591813484828.\n","------ epoch 172 -----\n","172 loss:0.10063655651771487.\n","Validating...\n","------ epoch 173 -----\n","173 loss:0.09912098797324759.\n","------ epoch 174 -----\n","174 loss:0.11009379708144464.\n","Validating...\n","------ epoch 175 -----\n","175 loss:0.11378449059895471.\n","------ epoch 176 -----\n","176 loss:0.10580916666131243.\n","Validating...\n","------ epoch 177 -----\n","177 loss:0.10607698840749824.\n","------ epoch 178 -----\n","178 loss:0.11575995268634497.\n","Validating...\n","------ epoch 179 -----\n","179 loss:0.11487383572941917.\n","------ epoch 180 -----\n","180 loss:0.10978767488342829.\n","Validating...\n","------ epoch 181 -----\n","181 loss:0.1093379210632963.\n","------ epoch 182 -----\n","182 loss:0.1017437313651491.\n","Validating...\n","------ epoch 183 -----\n"],"name":"stdout"}]}]}