{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TSceptionBandInput.ipynb","provenance":[{"file_id":"1EYz0RDU00kpyR70LFTuucwD2tc_ry_fQ","timestamp":1618074355729}],"mount_file_id":"1MWzJBL19OlSV7mf1T91V5AMZNPLCegLe","authorship_tag":"ABX9TyOFqekH/CyhgDgjZEk9x6Z4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CL_MTaBMQ9-p","executionInfo":{"status":"ok","timestamp":1618753785693,"user_tz":-60,"elapsed":478,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}},"outputId":"1f01ffc9-9fb5-4db8-9280-44d29910288b"},"source":["%cd /content/drive/MyDrive/\n","# raw_data is imported from global config\n","root_dir='/content/drive/MyDrive/'  # ChangeThis\n","result_dir=root_dir+'grasp/TSception/result_test_dropout/'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fiU4GcMWzaFT"},"source":["import sys, importlib\n","importlib.reload(sys.modules['grasp.config'])\n","from grasp.config import data_dir\n","# orveride the data_dir in config file\n","#data_dir='/content/drive/MyDrive/data/' # googleDrive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"N4zjc3vuPTZJ","executionInfo":{"status":"ok","timestamp":1618753785999,"user_tz":-60,"elapsed":768,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}},"outputId":"ce3fcc40-ae43-4b88-d29f-e70400958212"},"source":["data_dir"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/data/'"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xwROBJfQRAYe","executionInfo":{"status":"ok","timestamp":1618753790963,"user_tz":-60,"elapsed":5730,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}},"outputId":"9d80ce35-2513-412e-8ad3-87542ec82fb1"},"source":["! pip install mne==0.19.2;\n","! pip install torch;"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: mne==0.19.2 in /usr/local/lib/python3.7/dist-packages (0.19.2)\n","Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from mne==0.19.2) (1.4.1)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from mne==0.19.2) (1.19.5)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BQOkCPEnRJ7G"},"source":["import torch\n","import numpy as np\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","\n","from torch.utils.data import DataLoader\n","\n","from grasp.utils import regulization\n","from grasp.utils import SEEGDataset\n","from grasp.TSception.Models import TSception\n","\n","# load the data: regression to target force derivative\n","from grasp.utils import rawData2\n","from grasp.config import activeChannels, root_dir\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O01RtPYIVdvd"},"source":["# reload function\n","import sys, importlib\n","importlib.reload(sys.modules['grasp.TSception.Models'])\n","importlib.reload(sys.modules['grasp.utils'])\n","from grasp.TSception.Models import TSception,TSception2\n","from grasp.utils import rawData2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ppXUCroiY2iC"},"source":["# import from another folder\n","import sys\n","sys.path.insert(1, '/content/drive/MyDrive/examples')\n","from IMV_LSTM.networks import IMVTensorLSTM"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Dt_Bz4ztzear","executionInfo":{"status":"ok","timestamp":1618753790967,"user_tz":-60,"elapsed":5709,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}},"outputId":"470e078a-25ab-4603-8bdc-b3cb1280b4ca"},"source":["data_dir"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/data/'"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"1IjnSRE7WSZt"},"source":["import inspect as i\n","import sys\n","sys.stdout.write(i.getsource(rawData2));"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-BjdQBwjTddA","executionInfo":{"status":"ok","timestamp":1618753790968,"user_tz":-60,"elapsed":5699,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}},"outputId":"42f782c7-bdf6-4256-9894-f35db4ba27ac"},"source":["enable_cuda = torch.cuda.is_available()\n","print('GPU computing: ', enable_cuda)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["GPU computing:  True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VRvPvrrXZaOv"},"source":["%%capture\n","# suppress the output\n","sampling_rate=1000\n","#traindata, valdata, testdata = rawData2('raw',activeChannels,move2=False)  # (chns, 15000/15001, 118) (channels, time, trials)\n","traindata, valdata, testdata = rawData2('band','all',move2=True)\n","traindata = traindata.transpose(2, 0, 1)  # (118, 20, 15000) (trials,channels,  time)\n","valdata = valdata.transpose(2, 0, 1) # (8, 20, 15000)\n","testdata = testdata.transpose(2, 0, 1)  # (8, 20, 15000)\n","trainx, trainy = traindata[:, :-2, :], traindata[:, -2, :] #-2 is real force, -1 is target\n","valx, valy = valdata[:, :-2, :], valdata[:, -2, :]\n","testx, testy = testdata[:, -2, :], testdata[:, -2, :]\n","\n","dataset_train = SEEGDataset(trainx, trainy)\n","dataset_val = SEEGDataset(valx, valy)\n","dataset_test = SEEGDataset(testx, testy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6CIQaf0N63aE"},"source":["train_loader = DataLoader(dataset=dataset_train, batch_size=1, shuffle=True, pin_memory=False)\n","val_loader = DataLoader(dataset=dataset_val, batch_size=1, pin_memory=False)\n","test_loader = DataLoader(dataset=dataset_test, batch_size=1, pin_memory=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2bYgehRWw3YY"},"source":["chnNum=trainx.shape[1]\n","learning_rate=0.002\n","epochs=100\n","step=50 #ms\n","T=1000 #ms\n","totalLen=trainx.shape[2] #ms\n","batch_size=int((totalLen-T)/step) # 280\n","num_T = 3 # (6 conv2d layers) * ( 3 kernel each layer)\n","num_S = 3\n","hidden_size=222\n","dropout=0.2\n","Lambda = 1e-6\n","\n","# __init__(self,input_size, sampling_rate, num_T, num_S, hiden, dropout_rate)\n","net = TSception2(sampling_rate,chnNum, num_T, num_S,batch_size).float()\n","optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n","criterion = nn.MSELoss()\n","if(enable_cuda):\n","\tnet.cuda()\n"," \n","checkpoint = torch.load(result_dir+'checkpoint195.pth')\n","net.load_state_dict(checkpoint['state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jLXjw3Ns0EKX"},"source":["range(36,100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WFHqxkMIQCEV"},"source":["net"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"124r0ZZh94K8","executionInfo":{"status":"ok","timestamp":1618801879186,"user_tz":-60,"elapsed":23325291,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}},"outputId":"70633c26-f2a1-4f83-b03f-1edc7c5ff397"},"source":["debugg=False\n","#debugg=True\n","for epoch in range(196,400):\n","    print(\"------ epoch \" + str(epoch) + \" -----\")\n","    net.train()\n","\n","    loss_epoch=0\n","    #trial=0\n","    for trial, (trainx, trainy) in enumerate(train_loader): # ([1, 15000, 19]), ([1, 15000])\n","        #trainy[0,-1]+=0.05\n","        if debugg==True: # just test one trial\n","            if trial == 1:\n","                break\n","                pass\n","        optimizer.zero_grad()\n","        #print(\"Training on trial \" + str(trial) + \".\")\n","\n","        x = np.zeros((batch_size, 1, chnNum, T)) # 4D:(280,1,19,1000ms):(batch_size, planes, height, weight)\n","        targetd = np.zeros((batch_size,1)) # (280, 1)\n","        target = np.zeros((batch_size, 1))  # (280, 1)\n","\n","        # format 1 trial into 3D tensor\n","        # result: regress to force derative not good at all\n","        bs=0\n","        for bs in range(batch_size):\n","            x[bs, 0, :, :] = trainx[0, :, bs*step:(bs*step + T)]\n","            target[bs, 0] = trainy[0, bs * step + T + 1] # force\n","            targetd[bs,0] = abs(trainy[0,bs*step + T +1] - trainy[0,bs*step + T -50])*10+0.05 # force derative\n","        targetd[:,0] = [abs(item) / 5 if abs(item) > 0.2 else abs(item) for item in targetd[:,0]]\n","        if (enable_cuda):\n","            x= torch.from_numpy(x).float().cuda()\n","            target = torch.from_numpy(target).cuda()\n","        y_pred = net(x)\n","        #target = torch.from_numpy(target)\n","\n","        # regularization\n","        loss1 = criterion(y_pred, target.float())\n","        loss2 = regulization(net, Lambda)\n","        #loss3 = y_pred.cpu().detach().numpy()\n","        #loss3 = np.std(np.diff(loss3.reshape(-1)))\n","        loss=loss1+loss2 #+loss3*0.001\n","        loss.backward()\n","        optimizer.step()\n","\n","        ls=loss1.item()\n","        loss_epoch+=ls\n","        with open(result_dir+ \"trainlose.txt\", \"a\") as f:\n","            f.write(str(loss1) + \"\\n\")\n","    print(\"\"+str(epoch)+\" loss:\"+str(loss_epoch/(trial+1))+\".\")\n","    if epoch % 1 ==0:\n","        net.eval()\n","        print(\"Validating...\")\n","        with torch.no_grad():\n","            vpredAll = []\n","            vtargetAll = []\n","            for trial, (valx, valy) in enumerate(val_loader):  # ([1, 15000, 19]), ([1, 15000])\n","                \n","                valy[-1]+=0.05\n","                #print(\"Validating on trial \" + str(trial) + \".\")\n","\n","                vx = np.zeros((batch_size, 1, chnNum, T))  # 4D:(?,1,19,1000ms):(batch_size, planes, height, weight)\n","                vtarget = np.zeros((batch_size, 1))\n","                vtargetd = np.zeros((batch_size, 1))\n","\n","                # format 1 trial into 3D tensor\n","                for bs in range(batch_size):\n","                    vx[bs, 0, :, :] = valx[0, :, (bs * step):(bs * step + T)]\n","                    vtarget[bs, 0] = valy[0, bs * step + T + 1]\n","                    vtargetd[bs, 0] = abs(valy[0,bs * step + T + 1] - valy[0,bs * step + T - 50])*10 + 0.05\n","                vtargetd[:, 0] = [abs(item) / 5 if abs(item) > 0.5 else abs(item) for item in vtargetd[:, 0]]\n","                \n","                if (enable_cuda):\n","                    vx= torch.from_numpy(vx).float().cuda()\n","                    vtarget = torch.from_numpy(vtarget).cuda()\n","                y_pred = net(vx)\n","                \n","                loss3 = criterion(y_pred, vtarget.float())\n","                with open(result_dir+\"testlose.txt\", \"a\") as f:\n","                    f.write(str(loss3) + \"\\n\")\n","                \n","                vpredAll.append(y_pred.cpu().data.numpy())\n","                vtargetAll.append(vtarget.cpu().data.numpy())\n","\n","        vpredAll = np.concatenate(vpredAll,axis=0)\n","        vtargetAll = np.concatenate(vtargetAll, axis=0)\n","\n","        fig, ax = plt.subplots(figsize=(6, 3))\n","        plt.ion()\n","        ax.clear()\n","        ax.plot(vtargetAll, label=\"True\", linewidth=1)\n","        ax.plot(vpredAll, label='Predicted - Test', linewidth=1)\n","        ax.legend(loc='upper left')\n","        figname = result_dir+'prediction' + str(epoch) + '.png'\n","        fig.savefig(figname)\n","        plt.close(fig)\n","    if epoch % 5==0:\n","        state = {\n","            'state_dict': net.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","        }\n","        savepath = result_dir+'checkpoint'+str(epoch)+'.pth'\n","        torch.save(state, savepath)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["------ epoch 196 -----\n","196 loss:0.04506090300362873.\n","Validating...\n","------ epoch 197 -----\n","197 loss:0.043044654261794386.\n","Validating...\n","------ epoch 198 -----\n","198 loss:0.0409478681572413.\n","Validating...\n","------ epoch 199 -----\n","199 loss:0.04064991454183424.\n","Validating...\n","------ epoch 200 -----\n","200 loss:0.04070504601271349.\n","Validating...\n","------ epoch 201 -----\n","201 loss:0.040190042955485186.\n","Validating...\n","------ epoch 202 -----\n","202 loss:0.03797248730009769.\n","Validating...\n","------ epoch 203 -----\n","203 loss:0.04097911793643893.\n","Validating...\n","------ epoch 204 -----\n","204 loss:0.04132665795179368.\n","Validating...\n","------ epoch 205 -----\n","205 loss:0.03863808224214462.\n","Validating...\n","------ epoch 206 -----\n","206 loss:0.03861784766491313.\n","Validating...\n","------ epoch 207 -----\n","207 loss:0.03787849612249914.\n","Validating...\n","------ epoch 208 -----\n","208 loss:0.03768188757742992.\n","Validating...\n","------ epoch 209 -----\n","209 loss:0.03706350176342589.\n","Validating...\n","------ epoch 210 -----\n","210 loss:0.036681715655535206.\n","Validating...\n","------ epoch 211 -----\n","211 loss:0.03792168884695965.\n","Validating...\n","------ epoch 212 -----\n","212 loss:0.0362055920088127.\n","Validating...\n","------ epoch 213 -----\n","213 loss:0.0362005239605146.\n","Validating...\n","------ epoch 214 -----\n","214 loss:0.03674322206100796.\n","Validating...\n","------ epoch 215 -----\n","215 loss:0.037370192056711195.\n","Validating...\n","------ epoch 216 -----\n","216 loss:0.03883003134091022.\n","Validating...\n","------ epoch 217 -----\n","217 loss:0.03895405624693228.\n","Validating...\n","------ epoch 218 -----\n","218 loss:0.03766482326170524.\n","Validating...\n","------ epoch 219 -----\n","219 loss:0.03852077143787706.\n","Validating...\n","------ epoch 220 -----\n","220 loss:0.039829565508101704.\n","Validating...\n","------ epoch 221 -----\n","221 loss:0.04061896368955909.\n","Validating...\n","------ epoch 222 -----\n","222 loss:0.04220290996639405.\n","Validating...\n","------ epoch 223 -----\n","223 loss:0.04103348645286919.\n","Validating...\n","------ epoch 224 -----\n","224 loss:0.03935025468216104.\n","Validating...\n","------ epoch 225 -----\n","225 loss:0.041749839820914854.\n","Validating...\n","------ epoch 226 -----\n","226 loss:0.038945089512646705.\n","Validating...\n","------ epoch 227 -----\n","227 loss:0.03815702768437312.\n","Validating...\n","------ epoch 228 -----\n","228 loss:0.03813897905145156.\n","Validating...\n","------ epoch 229 -----\n","229 loss:0.03609731088597643.\n","Validating...\n","------ epoch 230 -----\n","230 loss:0.03741161980255807.\n","Validating...\n","------ epoch 231 -----\n","231 loss:0.03591364249587059.\n","Validating...\n","------ epoch 232 -----\n","232 loss:0.03594476016538249.\n","Validating...\n","------ epoch 233 -----\n","233 loss:0.03509906573631501.\n","Validating...\n","------ epoch 234 -----\n","234 loss:0.03599163276238083.\n","Validating...\n","------ epoch 235 -----\n","235 loss:0.036619616435619735.\n","Validating...\n","------ epoch 236 -----\n","236 loss:0.036054086622829405.\n","Validating...\n","------ epoch 237 -----\n","237 loss:0.03588230185747399.\n","Validating...\n","------ epoch 238 -----\n","238 loss:0.03513039373082377.\n","Validating...\n","------ epoch 239 -----\n","239 loss:0.03477065090358383.\n","Validating...\n","------ epoch 240 -----\n","240 loss:0.03397140196540346.\n","Validating...\n","------ epoch 241 -----\n","241 loss:0.0348125784342193.\n","Validating...\n","------ epoch 242 -----\n","242 loss:0.0351322057254423.\n","Validating...\n","------ epoch 243 -----\n","243 loss:0.03625340545076435.\n","Validating...\n","------ epoch 244 -----\n","244 loss:0.0353602513876128.\n","Validating...\n","------ epoch 245 -----\n","245 loss:0.03271303430073342.\n","Validating...\n","------ epoch 246 -----\n","246 loss:0.03482128677384581.\n","Validating...\n","------ epoch 247 -----\n","247 loss:0.03468557751453403.\n","Validating...\n","------ epoch 248 -----\n","248 loss:0.033811636675559614.\n","Validating...\n","------ epoch 249 -----\n","249 loss:0.0345099731782546.\n","Validating...\n","------ epoch 250 -----\n","250 loss:0.034680033075007596.\n","Validating...\n","------ epoch 251 -----\n","251 loss:0.032947488965750754.\n","Validating...\n","------ epoch 252 -----\n","252 loss:0.03301579174901343.\n","Validating...\n","------ epoch 253 -----\n","253 loss:0.03944975899343016.\n","Validating...\n","------ epoch 254 -----\n","254 loss:0.04082890737691294.\n","Validating...\n","------ epoch 255 -----\n","255 loss:0.034739570475136074.\n","Validating...\n","------ epoch 256 -----\n","256 loss:0.0345437960438746.\n","Validating...\n","------ epoch 257 -----\n","257 loss:0.03253626430770224.\n","Validating...\n","------ epoch 258 -----\n","258 loss:0.03265043294799151.\n","Validating...\n","------ epoch 259 -----\n","259 loss:0.033326979426635525.\n","Validating...\n","------ epoch 260 -----\n","260 loss:0.03357485845542939.\n","Validating...\n","------ epoch 261 -----\n","261 loss:0.03335228942776636.\n","Validating...\n","------ epoch 262 -----\n","262 loss:0.03285650684008912.\n","Validating...\n","------ epoch 263 -----\n","263 loss:0.03196130471572406.\n","Validating...\n","------ epoch 264 -----\n","264 loss:0.03257272223610494.\n","Validating...\n","------ epoch 265 -----\n","265 loss:0.03306180917878904.\n","Validating...\n","------ epoch 266 -----\n","266 loss:0.0341670125036232.\n","Validating...\n","------ epoch 267 -----\n","267 loss:0.0360022823610422.\n","Validating...\n","------ epoch 268 -----\n","268 loss:0.032415161296030726.\n","Validating...\n","------ epoch 269 -----\n","269 loss:0.031665243208408356.\n","Validating...\n","------ epoch 270 -----\n","270 loss:0.033111768451093114.\n","Validating...\n","------ epoch 271 -----\n","271 loss:0.032537401665665086.\n","Validating...\n","------ epoch 272 -----\n","272 loss:0.0305411347614254.\n","Validating...\n","------ epoch 273 -----\n","273 loss:0.032091435771119795.\n","Validating...\n","------ epoch 274 -----\n","274 loss:0.03248724051340783.\n","Validating...\n","------ epoch 275 -----\n","275 loss:0.030719544221568158.\n","Validating...\n","------ epoch 276 -----\n","276 loss:0.03340144219050594.\n","Validating...\n","------ epoch 277 -----\n","277 loss:0.03185052257741533.\n","Validating...\n","------ epoch 278 -----\n","278 loss:0.03204004702126702.\n","Validating...\n","------ epoch 279 -----\n","279 loss:0.03300329059425552.\n","Validating...\n","------ epoch 280 -----\n","280 loss:0.03304133565812293.\n","Validating...\n","------ epoch 281 -----\n","281 loss:0.03235054211059617.\n","Validating...\n","------ epoch 282 -----\n","282 loss:0.030699337142030315.\n","Validating...\n","------ epoch 283 -----\n","283 loss:0.030986729508139573.\n","Validating...\n","------ epoch 284 -----\n","284 loss:0.02986878295571117.\n","Validating...\n","------ epoch 285 -----\n","285 loss:0.030333798256371232.\n","Validating...\n","------ epoch 286 -----\n","286 loss:0.031141069575639096.\n","Validating...\n","------ epoch 287 -----\n","287 loss:0.033436014249889276.\n","Validating...\n","------ epoch 288 -----\n","288 loss:0.03317626515971655.\n","Validating...\n","------ epoch 289 -----\n","289 loss:0.032733249663517386.\n","Validating...\n","------ epoch 290 -----\n","290 loss:0.033842685786312665.\n","Validating...\n","------ epoch 291 -----\n","291 loss:0.03355764855741192.\n","Validating...\n","------ epoch 292 -----\n","292 loss:0.03235103956707832.\n","Validating...\n","------ epoch 293 -----\n","293 loss:0.038085391042533064.\n","Validating...\n","------ epoch 294 -----\n","294 loss:0.03568477193446104.\n","Validating...\n","------ epoch 295 -----\n","295 loss:0.03493455130906807.\n","Validating...\n","------ epoch 296 -----\n","296 loss:0.03326549042032053.\n","Validating...\n","------ epoch 297 -----\n","297 loss:0.0345871755261353.\n","Validating...\n","------ epoch 298 -----\n","298 loss:0.03192855635563196.\n","Validating...\n","------ epoch 299 -----\n","299 loss:0.03373027761009032.\n","Validating...\n","------ epoch 300 -----\n","300 loss:0.03408628369886744.\n","Validating...\n","------ epoch 301 -----\n","301 loss:0.03352033647992727.\n","Validating...\n","------ epoch 302 -----\n","302 loss:0.03275407936376661.\n","Validating...\n","------ epoch 303 -----\n","303 loss:0.03382700111843267.\n","Validating...\n","------ epoch 304 -----\n","304 loss:0.03348009107571285.\n","Validating...\n","------ epoch 305 -----\n","305 loss:0.031901587953128044.\n","Validating...\n","------ epoch 306 -----\n","306 loss:0.0325109968474924.\n","Validating...\n","------ epoch 307 -----\n","307 loss:0.03297407184017159.\n","Validating...\n","------ epoch 308 -----\n","308 loss:0.03321766571962606.\n","Validating...\n","------ epoch 309 -----\n","309 loss:0.03209956661577068.\n","Validating...\n","------ epoch 310 -----\n","310 loss:0.03380496795210293.\n","Validating...\n","------ epoch 311 -----\n","311 loss:0.030673008518672343.\n","Validating...\n","------ epoch 312 -----\n","312 loss:0.031589407927625004.\n","Validating...\n","------ epoch 313 -----\n","313 loss:0.030527921953885737.\n","Validating...\n","------ epoch 314 -----\n","314 loss:0.030215228213218308.\n","Validating...\n","------ epoch 315 -----\n","315 loss:0.02892364031177456.\n","Validating...\n","------ epoch 316 -----\n","316 loss:0.029406826966836795.\n","Validating...\n","------ epoch 317 -----\n","317 loss:0.029723899731002118.\n","Validating...\n","------ epoch 318 -----\n","318 loss:0.029940692846016106.\n","Validating...\n","------ epoch 319 -----\n","319 loss:0.029021931729295243.\n","Validating...\n","------ epoch 320 -----\n","320 loss:0.029133137844313504.\n","Validating...\n","------ epoch 321 -----\n","321 loss:0.029089517256055596.\n","Validating...\n","------ epoch 322 -----\n","322 loss:0.0290566760123218.\n","Validating...\n","------ epoch 323 -----\n","323 loss:0.031107308761358768.\n","Validating...\n","------ epoch 324 -----\n","324 loss:0.02770178882657724.\n","Validating...\n","------ epoch 325 -----\n","325 loss:0.02826326733486632.\n","Validating...\n","------ epoch 326 -----\n","326 loss:0.028661118705867458.\n","Validating...\n","------ epoch 327 -----\n","327 loss:0.02755664861208554.\n","Validating...\n","------ epoch 328 -----\n","328 loss:0.02843140783893355.\n","Validating...\n","------ epoch 329 -----\n","329 loss:0.02917253648688606.\n","Validating...\n","------ epoch 330 -----\n","330 loss:0.027844229598640132.\n","Validating...\n","------ epoch 331 -----\n","331 loss:0.027281753091394145.\n","Validating...\n","------ epoch 332 -----\n","332 loss:0.02805128625309947.\n","Validating...\n","------ epoch 333 -----\n","333 loss:0.02707834144801659.\n","Validating...\n","------ epoch 334 -----\n","334 loss:0.028157458423589497.\n","Validating...\n","------ epoch 335 -----\n","335 loss:0.028150030139486415.\n","Validating...\n","------ epoch 336 -----\n","336 loss:0.02820399431196057.\n","Validating...\n","------ epoch 337 -----\n","337 loss:0.02803064284498914.\n","Validating...\n","------ epoch 338 -----\n","338 loss:0.026273308992701565.\n","Validating...\n","------ epoch 339 -----\n","339 loss:0.026015899144113064.\n","Validating...\n","------ epoch 340 -----\n","340 loss:0.02558915186831254.\n","Validating...\n","------ epoch 341 -----\n","341 loss:0.034509136893991696.\n","Validating...\n","------ epoch 342 -----\n","342 loss:0.030061280564010397.\n","Validating...\n","------ epoch 343 -----\n","343 loss:0.025934773958998465.\n","Validating...\n","------ epoch 344 -----\n","344 loss:0.026251569504739117.\n","Validating...\n","------ epoch 345 -----\n","345 loss:0.02687597147315362.\n","Validating...\n","------ epoch 346 -----\n","346 loss:0.03370857792038281.\n","Validating...\n","------ epoch 347 -----\n","347 loss:0.03133802608018583.\n","Validating...\n","------ epoch 348 -----\n","348 loss:0.029423952461772804.\n","Validating...\n","------ epoch 349 -----\n","349 loss:0.030208881115698714.\n","Validating...\n","------ epoch 350 -----\n","350 loss:0.028896365168709623.\n","Validating...\n","------ epoch 351 -----\n","351 loss:0.029291756352472862.\n","Validating...\n","------ epoch 352 -----\n","352 loss:0.029065319806558347.\n","Validating...\n","------ epoch 353 -----\n","353 loss:0.030147138343713548.\n","Validating...\n","------ epoch 354 -----\n","354 loss:0.029094212207059236.\n","Validating...\n","------ epoch 355 -----\n","355 loss:0.029373895148036337.\n","Validating...\n","------ epoch 356 -----\n","356 loss:0.03021438372068865.\n","Validating...\n","------ epoch 357 -----\n","357 loss:0.02961596147299318.\n","Validating...\n","------ epoch 358 -----\n","358 loss:0.028378257638443323.\n","Validating...\n","------ epoch 359 -----\n","359 loss:0.027760787815707972.\n","Validating...\n","------ epoch 360 -----\n","360 loss:0.029511279907961516.\n","Validating...\n","------ epoch 361 -----\n","361 loss:0.027454492793876236.\n","Validating...\n","------ epoch 362 -----\n","362 loss:0.02593949053957427.\n","Validating...\n","------ epoch 363 -----\n","363 loss:0.026312585549144927.\n","Validating...\n","------ epoch 364 -----\n","364 loss:0.026813297193118577.\n","Validating...\n","------ epoch 365 -----\n","365 loss:0.027534324995447265.\n","Validating...\n","------ epoch 366 -----\n","366 loss:0.02620984361333362.\n","Validating...\n","------ epoch 367 -----\n","367 loss:0.02726083579157495.\n","Validating...\n","------ epoch 368 -----\n","368 loss:0.028551224607311316.\n","Validating...\n","------ epoch 369 -----\n","369 loss:0.027989538279914503.\n","Validating...\n","------ epoch 370 -----\n","370 loss:0.025986515782710353.\n","Validating...\n","------ epoch 371 -----\n","371 loss:0.02772220612850861.\n","Validating...\n","------ epoch 372 -----\n","372 loss:0.0291715741189102.\n","Validating...\n","------ epoch 373 -----\n","373 loss:0.032982832026052275.\n","Validating...\n","------ epoch 374 -----\n","374 loss:0.036074238462310476.\n","Validating...\n","------ epoch 375 -----\n","375 loss:0.03148818371612263.\n","Validating...\n","------ epoch 376 -----\n","376 loss:0.030187290082922428.\n","Validating...\n","------ epoch 377 -----\n","377 loss:0.029210329553017677.\n","Validating...\n","------ epoch 378 -----\n","378 loss:0.03706934740293329.\n","Validating...\n","------ epoch 379 -----\n","379 loss:0.030591196416072167.\n","Validating...\n","------ epoch 380 -----\n","380 loss:0.029444435812627612.\n","Validating...\n","------ epoch 381 -----\n","381 loss:0.031835433707367314.\n","Validating...\n","------ epoch 382 -----\n","382 loss:0.027951579075306654.\n","Validating...\n","------ epoch 383 -----\n","383 loss:0.0284845329726399.\n","Validating...\n","------ epoch 384 -----\n","384 loss:0.027874456836952496.\n","Validating...\n","------ epoch 385 -----\n","385 loss:0.026487779464193825.\n","Validating...\n","------ epoch 386 -----\n","386 loss:0.026755554025392918.\n","Validating...\n","------ epoch 387 -----\n","387 loss:0.027841851825557522.\n","Validating...\n","------ epoch 388 -----\n","388 loss:0.031555238444263396.\n","Validating...\n","------ epoch 389 -----\n","389 loss:0.03737188129038629.\n","Validating...\n","------ epoch 390 -----\n","390 loss:0.031316656398318585.\n","Validating...\n","------ epoch 391 -----\n","391 loss:0.026993341100702093.\n","Validating...\n","------ epoch 392 -----\n","392 loss:0.029500625715827792.\n","Validating...\n","------ epoch 393 -----\n","393 loss:0.02992297809223755.\n","Validating...\n","------ epoch 394 -----\n","394 loss:0.027687225179052203.\n","Validating...\n","------ epoch 395 -----\n","395 loss:0.03114601555359313.\n","Validating...\n","------ epoch 396 -----\n","396 loss:0.028514983211407215.\n","Validating...\n","------ epoch 397 -----\n","397 loss:0.03342083038888493.\n","Validating...\n","------ epoch 398 -----\n","398 loss:0.031729068113806644.\n","Validating...\n","------ epoch 399 -----\n","399 loss:0.03000244418143342.\n","Validating...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G9Ml_yK3Wvpp"},"source":[""],"execution_count":null,"outputs":[]}]}