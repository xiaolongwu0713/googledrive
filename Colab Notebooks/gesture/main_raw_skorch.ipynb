{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"main_raw_skorch.ipynb","provenance":[{"file_id":"1EYz0RDU00kpyR70LFTuucwD2tc_ry_fQ","timestamp":1618074355729}],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1uCtNDUOCXLXKoceHyU_7e44Rpr9YsuGq","authorship_tag":"ABX9TyOQosxmh4dWqswgx0kv+kvF"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CL_MTaBMQ9-p","executionInfo":{"elapsed":449,"status":"ok","timestamp":1631257126518,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"},"user_tz":-480},"outputId":"7d325c08-2f81-4b93-9c2e-b3505701ac4c"},"source":["%cd /content/drive/MyDrive/\n","# raw_data is imported from global config"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive\n"]}]},{"cell_type":"code","metadata":{"id":"xwROBJfQRAYe"},"source":["%%capture\n","! pip install hdf5storage\n","! pip install mne==0.23.0\n","! pip install torch\n","! pip install Braindecode==0.5.1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9NQVV2AYNaLO"},"source":["***Copy and Paste your code below.***"]},{"cell_type":"code","metadata":{"id":"W2RjuTz5T_ez"},"source":["import os, re\n","import hdf5storage\n","import numpy as np\n","from scipy.io import savemat\n","from sklearn.model_selection import StratifiedKFold\n","import matplotlib.pyplot as plt\n","from braindecode.datautil import (create_from_mne_raw, create_from_mne_epochs)\n","import torch\n","from braindecode.util import set_random_seeds\n","from skorch.callbacks import LRScheduler\n","from skorch.helper import predefined_split\n","from braindecode import EEGClassifier\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","\n","from braindecode.models import ShallowFBCSPNet,EEGNetv4,Deep4Net\n","from gesture.models.deepmodel import deepnet,deepnet_resnet\n","from gesture.models.d2l_resnet import d2lresnet\n","from gesture.models.EEGModels import DeepConvNet_210519_512_10\n","from gesture.models.tsception import TSception\n","\n","from gesture.myskorch import on_epoch_begin_callback, on_batch_end_callback\n","from gesture.config import *\n","from gesture.preprocess.chn_settings import get_channel_setting\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q9or620gc2Bk"},"source":["import inspect as i\n","import sys\n","#sys.stdout.write(i.getsource(deepnet))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EzULpJKf39Sn"},"source":["#a=torch.randn(1, 1, 208, 500)\n","#model = deepnet_resnet(208,5,input_window_samples=500,expand=False)\n","#model.train()\n","#b=model(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Svy7ABlg3wuE"},"source":["pn=10 #4\n","Session_num,UseChn,EmgChn,TrigChn, activeChan = get_channel_setting(pn)\n","#fs=[Frequencies[i,1] for i in range(Frequencies.shape[0]) if Frequencies[i,0] == pn][0]\n","fs=1000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_np8cCh3reZN","executionInfo":{"elapsed":11,"status":"ok","timestamp":1631257141656,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"},"user_tz":-480},"outputId":"2bdfc950-94c7-4c6a-d7ee-0f1e1c23e693"},"source":["[Frequencies[i,1] for i in range(Frequencies.shape[0]) if Frequencies[i,0] == pn][0]"],"execution_count":null,"outputs":[{"data":{"text/plain":["2000"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"1LBbiAuvUVb_"},"source":["loadPath = data_dir+'preprocessing'+'/P'+str(pn)+'/preprocessing2.mat'\n","mat=hdf5storage.loadmat(loadPath)\n","data = mat['Datacell']\n","channelNum=int(mat['channelNum'][0,0])\n","data=np.concatenate((data[0,0],data[0,1]),0)\n","del mat\n","# standardization\n","# no effect. why?\n","chn_data=data[:,-3:]\n","data=data[:,:-3]\n","scaler = StandardScaler()\n","scaler.fit(data)\n","data=scaler.transform((data))\n","data=np.concatenate((data,chn_data),axis=1)\n","\n","# stim0 is trigger channel, stim1 is trigger position calculated from EMG signal.\n","chn_names=np.append([\"seeg\"]*len(UseChn),[\"stim0\", \"emg\",\"stim1\"])\n","chn_types=np.append([\"seeg\"]*len(UseChn),[\"stim\", \"emg\",\"stim\"])\n","info = mne.create_info(ch_names=list(chn_names), ch_types=list(chn_types), sfreq=fs)\n","raw = mne.io.RawArray(data.transpose(), info)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NxbH3k2Ej3SF"},"source":["# gesture/events type: 1,2,3,4,5\n","events0 = mne.find_events(raw, stim_channel='stim0')\n","events1 = mne.find_events(raw, stim_channel='stim1')\n","# events number should start from 0: 0,1,2,3,4, instead of 1,2,3,4,5\n","events0=events0-[0,0,1]\n","events1=events1-[0,0,1]\n","\n","#print(events[:5])  # show the first 5\n","# Epoch from 4s before(idle) until 4s after(movement) stim1.\n","raw=raw.pick([\"seeg\"])\n","epochs = mne.Epochs(raw, events1, tmin=0, tmax=4,baseline=None)\n","# or epoch from 0s to 4s which only contain movement data.\n","# epochs = mne.Epochs(raw, events1, tmin=0, tmax=4,baseline=None)\n","\n","epoch1=epochs['0'] # 20 trials. 8001 time points per trial for 8s.\n","epoch2=epochs['1']\n","epoch3=epochs['2']\n","epoch4=epochs['3']\n","epoch5=epochs['4']\n","list_of_epochs=[epoch1,epoch2,epoch3,epoch4,epoch5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"edBaRd88j-DC"},"source":["#note: windows_datasets is of class BaseConcatDataset. windows_datasets.datasets is a list of all\n","# trials (like an epoch but organized as a list) epoched from a run.\n","#windows_datasets.datasets[0].windows is an epoch again created by a sliding window from one trial.\n","\n","\n","# 20 trials/epoch * 5 epochs =100 trials=100 datasets\n","# 1 dataset can be slided into ~161(depends on wind_size and stride) windows.\n","windows_datasets = create_from_mne_epochs(\n","    list_of_epochs,\n","    window_size_samples=500,\n","    window_stride_samples=250,\n","    drop_last_window=False\n",")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WniFNShFkQwJ"},"source":["# train/valid/test split based on description column\n","desc=windows_datasets.description\n","desc=desc.rename(columns={0: 'split'})\n","trials_per_epoch=epoch1.events.shape[0] # 20 trial per epoch list/class\n","import random\n","val_test_num=2 # two val and two test trials\n","random_index = random.sample(range(trials_per_epoch), val_test_num*2)\n","sorted(random_index)\n","val_index=[rand+iclass*20 for iclass in range(5) for rand in sorted(random_index[:2]) ]\n","test_index=[rand+iclass*20 for iclass in range(5) for rand in sorted(random_index[-2:])]\n","train_index=[item for  item in list(range(100)) if item not in val_index+test_index]\n","desc.iloc[val_index]='validate'\n","desc.iloc[test_index]='test'\n","desc.iloc[train_index]='train'\n","# make sure there are val_test_num trials from each epoch (5 intotal) for both validate and test dataset\n","assert desc[desc['split'] == 'validate'].size == desc[desc['split'] == 'test'].size == val_test_num*5\n","windows_datasets.description=desc\n","splitted = windows_datasets.split('split')\n","\n","train_set = splitted['train']\n","valid_set = splitted['validate']\n","test_set = splitted['test']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SIIBAOLXkY2k"},"source":["cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n","device = 'cuda' if cuda else 'cpu'\n","if cuda:\n","    torch.backends.cudnn.benchmark = True\n","seed = 20200220  # random seed to make results reproducible\n","# Set random seed to be able to reproduce results\n","set_random_seeds(seed=seed, cuda=cuda)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nSrQqFFGkeK-","executionInfo":{"elapsed":17,"status":"ok","timestamp":1631257163443,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"},"user_tz":-480},"outputId":"7337ee75-51e3-4c4c-80b0-bd570146f4f1"},"source":["n_classes = 5\n","# Extract number of chans and time steps from dataset\n","one_window=windows_datasets.datasets[0].windows.get_data()\n","n_chans = one_window.shape[1]\n","input_window_samples = one_window.shape[2]\n","\n","#model = ShallowFBCSPNet(n_chans,n_classes,input_window_samples=input_window_samples,final_conv_length='auto',) # 51%\n","#model = EEGNetv4(n_chans,n_classes,input_window_samples=input_window_samples,final_conv_length='auto',)\n","\n","#model = deepnet(n_chans,n_classes,input_window_samples=input_window_samples,final_conv_length='auto',) # 85%\n","\n","#model = deepnet_resnet(n_chans,n_classes,input_window_samples=input_window_samples,expand=True) # 50%\n","\n","#model=d2lresnet() # 92%\n","\n","model=TSception(208)\n","\n","#model=TSception(1000,n_chans,3,3,0.5)\n","# Send model to GPU\n","if cuda:\n","    model.cuda()\n"],"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]}]},{"cell_type":"code","metadata":{"id":"icHBaZeXkn85"},"source":["# These values we found good for shallow network:\n","lr = 0.0001\n","weight_decay = 1e-10\n","batch_size = 32\n","n_epochs = 200"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9cJpa6_TZkLg"},"source":["location=os.getcwd()\n","if re.compile('/Users/long/').match(location):\n","    my_callbacks=[\n","        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n","        ('on_epoch_begin_callback', on_epoch_begin_callback),('on_batch_end_callback',on_batch_end_callback),\n","    ]\n","elif re.compile('/content/drive').match(location):\n","   my_callbacks=[\n","        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n","    ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"uf5hmYVAkr9i","outputId":"914cf312-f615-44eb-8e30-7322cd1f21c4"},"source":["clf = EEGClassifier(\n","    model,\n","    #criterion=torch.nn.NLLLoss,  #torch.nn.NLLLoss/CrossEntropyLoss\n","    criterion=torch.nn.CrossEntropyLoss,\n","    optimizer=torch.optim.Adam, #optimizer=torch.optim.AdamW,\n","    train_split=predefined_split(valid_set),  # using valid_set for validation; None means no validate:both train and test on training dataset.\n","    optimizer__lr=lr,\n","    optimizer__weight_decay=weight_decay,\n","    batch_size=batch_size,\n","    callbacks=my_callbacks,\n","    device=device,\n",")\n","# Model training for a specified number of epochs. `y` is None as it is already supplied\n","# in the dataset.\n","clf.fit(train_set, y=None, epochs=n_epochs)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr      dur\n","-------  ----------------  ------------  ----------------  ------------  ------  -------\n","      1            \u001b[36m0.2195\u001b[0m        \u001b[32m1.6095\u001b[0m            \u001b[35m0.1812\u001b[0m        \u001b[31m1.6168\u001b[0m  0.0001  96.2927\n","      2            0.2117        \u001b[32m1.6072\u001b[0m            0.1688        \u001b[31m1.6166\u001b[0m  0.0001  95.7629\n","      3            0.2172        \u001b[32m1.6058\u001b[0m            0.1562        \u001b[31m1.6165\u001b[0m  0.0001  95.4620\n","      4            \u001b[36m0.2266\u001b[0m        \u001b[32m1.6039\u001b[0m            0.1437        \u001b[31m1.6165\u001b[0m  0.0001  103.2181\n","      5            0.2258        \u001b[32m1.6019\u001b[0m            0.1688        \u001b[31m1.6165\u001b[0m  0.0001  98.3680\n","      6            \u001b[36m0.2313\u001b[0m        \u001b[32m1.6001\u001b[0m            0.1812        1.6166  0.0001  95.3136\n","      7            \u001b[36m0.2477\u001b[0m        \u001b[32m1.5982\u001b[0m            0.1500        1.6165  0.0001  95.3829\n","      8            0.2453        \u001b[32m1.5965\u001b[0m            0.1625        1.6166  0.0001  95.5336\n","      9            0.2477        \u001b[32m1.5944\u001b[0m            0.1500        1.6168  0.0001  95.2397\n","     10            \u001b[36m0.2508\u001b[0m        \u001b[32m1.5919\u001b[0m            0.1375        1.6172  0.0001  95.0892\n","     11            0.2500        \u001b[32m1.5894\u001b[0m            0.1562        1.6172  0.0001  95.2144\n","     12            0.2437        \u001b[32m1.5867\u001b[0m            0.1812        1.6178  0.0001  94.7326\n","     13            \u001b[36m0.2516\u001b[0m        \u001b[32m1.5833\u001b[0m            \u001b[35m0.1875\u001b[0m        1.6181  0.0001  94.9231\n","     14            \u001b[36m0.2562\u001b[0m        \u001b[32m1.5799\u001b[0m            0.1812        1.6187  0.0001  95.1328\n","     15            \u001b[36m0.2570\u001b[0m        \u001b[32m1.5761\u001b[0m            0.1750        1.6196  0.0001  95.4195\n","     16            0.2523        \u001b[32m1.5706\u001b[0m            0.1750        1.6202  0.0001  95.0038\n","     17            0.2570        \u001b[32m1.5651\u001b[0m            0.1812        1.6210  0.0001  95.4334\n","     18            \u001b[36m0.2609\u001b[0m        \u001b[32m1.5598\u001b[0m            \u001b[35m0.1938\u001b[0m        1.6233  0.0001  107.4944\n","     19            0.2602        \u001b[32m1.5530\u001b[0m            0.1750        1.6248  0.0001  94.1861\n","     20            \u001b[36m0.2711\u001b[0m        \u001b[32m1.5460\u001b[0m            0.1750        1.6282  0.0001  94.2375\n","     21            \u001b[36m0.2734\u001b[0m        \u001b[32m1.5375\u001b[0m            0.1688        1.6320  0.0001  93.6285\n","     22            0.2727        \u001b[32m1.5286\u001b[0m            0.1812        1.6367  0.0001  91.8716\n","     23            \u001b[36m0.2859\u001b[0m        \u001b[32m1.5197\u001b[0m            0.1750        1.6407  0.0001  89.6300\n","     24            \u001b[36m0.2898\u001b[0m        \u001b[32m1.5091\u001b[0m            0.1750        1.6462  0.0001  88.1870\n","     25            \u001b[36m0.2930\u001b[0m        \u001b[32m1.4981\u001b[0m            0.1750        1.6514  0.0001  89.6172\n","     26            \u001b[36m0.3039\u001b[0m        \u001b[32m1.4894\u001b[0m            0.1875        1.6576  0.0001  86.5005\n","     27            \u001b[36m0.3117\u001b[0m        \u001b[32m1.4799\u001b[0m            0.1875        1.6643  0.0001  86.2021\n","     28            \u001b[36m0.3219\u001b[0m        \u001b[32m1.4674\u001b[0m            \u001b[35m0.2125\u001b[0m        1.6645  0.0001  86.5103\n","     29            0.3211        \u001b[32m1.4541\u001b[0m            0.2062        1.6700  0.0001  86.2338\n","     30            \u001b[36m0.3359\u001b[0m        \u001b[32m1.4454\u001b[0m            0.1938        1.6715  0.0001  86.4709\n","     31            0.3305        \u001b[32m1.4302\u001b[0m            0.2062        1.6781  0.0001  86.0173\n","     32            \u001b[36m0.3422\u001b[0m        \u001b[32m1.4255\u001b[0m            0.2062        1.6779  0.0001  86.3616\n","     33            0.3398        \u001b[32m1.4088\u001b[0m            0.2125        1.6834  0.0001  88.6963\n","     34            \u001b[36m0.3586\u001b[0m        \u001b[32m1.4042\u001b[0m            0.2000        1.6887  0.0001  86.5665\n","     35            \u001b[36m0.3609\u001b[0m        \u001b[32m1.3929\u001b[0m            0.2125        1.6938  0.0001  92.3340\n","     36            \u001b[36m0.3648\u001b[0m        \u001b[32m1.3778\u001b[0m            \u001b[35m0.2250\u001b[0m        1.6970  0.0001  86.9122\n","     37            \u001b[36m0.3734\u001b[0m        \u001b[32m1.3681\u001b[0m            0.2000        1.7063  0.0001  86.8158\n","     38            \u001b[36m0.3758\u001b[0m        \u001b[32m1.3564\u001b[0m            0.2125        1.7167  0.0001  87.5764\n","     39            0.3750        \u001b[32m1.3486\u001b[0m            0.2125        1.7208  0.0001  86.7035\n","     40            \u001b[36m0.3859\u001b[0m        \u001b[32m1.3325\u001b[0m            0.2188        1.7233  0.0001  87.0545\n","     41            \u001b[36m0.3945\u001b[0m        \u001b[32m1.3280\u001b[0m            0.2062        1.7293  0.0001  86.2198\n","     42            \u001b[36m0.3969\u001b[0m        \u001b[32m1.3097\u001b[0m            0.2188        1.7397  0.0001  86.6006\n","     43            0.3937        \u001b[32m1.3030\u001b[0m            0.2188        1.7527  0.0001  86.7373\n","     44            \u001b[36m0.4078\u001b[0m        \u001b[32m1.2932\u001b[0m            0.2062        1.7590  0.0001  86.9211\n","     45            \u001b[36m0.4125\u001b[0m        \u001b[32m1.2796\u001b[0m            0.2250        1.7553  0.0001  86.4549\n","     46            \u001b[36m0.4133\u001b[0m        \u001b[32m1.2746\u001b[0m            0.2062        1.7578  0.0001  87.3362\n","     47            \u001b[36m0.4273\u001b[0m        \u001b[32m1.2697\u001b[0m            0.2125        1.7686  0.0001  85.9835\n","     48            \u001b[36m0.4328\u001b[0m        \u001b[32m1.2490\u001b[0m            0.2188        1.7705  0.0001  86.2640\n","     49            0.4266        \u001b[32m1.2417\u001b[0m            0.2125        1.7702  0.0001  86.2492\n","     50            0.4297        \u001b[32m1.2300\u001b[0m            0.2125        1.7815  0.0001  103.9509\n","     51            \u001b[36m0.4453\u001b[0m        \u001b[32m1.2196\u001b[0m            \u001b[35m0.2313\u001b[0m        1.7841  0.0001  89.5850\n","     52            \u001b[36m0.4523\u001b[0m        \u001b[32m1.2060\u001b[0m            \u001b[35m0.2437\u001b[0m        1.7978  0.0001  94.5321\n","     53            \u001b[36m0.4578\u001b[0m        \u001b[32m1.1994\u001b[0m            0.2250        1.8044  0.0001  85.9609\n","     54            0.4555        \u001b[32m1.1831\u001b[0m            0.2313        1.8182  0.0001  85.8286\n","     55            \u001b[36m0.4711\u001b[0m        \u001b[32m1.1786\u001b[0m            0.2313        1.8036  0.0001  87.4099\n","     56            \u001b[36m0.4734\u001b[0m        \u001b[32m1.1691\u001b[0m            0.2375        1.8091  0.0001  87.5041\n","     57            \u001b[36m0.4750\u001b[0m        \u001b[32m1.1509\u001b[0m            0.2125        1.8354  0.0001  88.2273\n","     58            0.4711        \u001b[32m1.1452\u001b[0m            0.2313        1.8380  0.0001  88.8058\n","     59            \u001b[36m0.4789\u001b[0m        \u001b[32m1.1348\u001b[0m            0.2250        1.8494  0.0001  88.4054\n","     60            \u001b[36m0.4875\u001b[0m        \u001b[32m1.1219\u001b[0m            0.2188        1.8511  0.0001  87.9972\n","     61            0.4859        \u001b[32m1.1210\u001b[0m            0.2188        1.8469  0.0001  115.7391\n","     62            \u001b[36m0.4898\u001b[0m        \u001b[32m1.1132\u001b[0m            0.2125        1.8703  0.0001  88.6536\n","     63            \u001b[36m0.4938\u001b[0m        \u001b[32m1.1030\u001b[0m            0.2188        1.8702  0.0001  88.5639\n","     64            \u001b[36m0.5062\u001b[0m        \u001b[32m1.0966\u001b[0m            0.2250        1.8655  0.0001  90.0692\n","     65            0.4922        \u001b[32m1.0872\u001b[0m            0.2062        1.9137  0.0001  97.2315\n","     66            0.4961        \u001b[32m1.0830\u001b[0m            0.2375        1.8964  0.0001  87.8101\n","     67            \u001b[36m0.5133\u001b[0m        \u001b[32m1.0705\u001b[0m            0.2313        1.8837  0.0001  88.4110\n","     68            0.5039        \u001b[32m1.0634\u001b[0m            0.2313        1.8983  0.0001  97.7747\n","     69            \u001b[36m0.5180\u001b[0m        \u001b[32m1.0614\u001b[0m            0.2375        1.9098  0.0001  96.6064\n","     70            \u001b[36m0.5203\u001b[0m        \u001b[32m1.0475\u001b[0m            0.2437        1.9166  0.0001  89.5866\n","     71            \u001b[36m0.5227\u001b[0m        \u001b[32m1.0349\u001b[0m            0.2125        1.9163  0.0001  93.3935\n","     72            \u001b[36m0.5242\u001b[0m        1.0378            0.2250        1.9207  0.0001  88.8760\n","     73            0.5180        \u001b[32m1.0191\u001b[0m            0.2250        1.9292  0.0001  87.8675\n","     74            0.5227        \u001b[32m1.0144\u001b[0m            0.2000        1.9294  0.0001  87.9852\n","     75            0.5188        1.0155            0.2188        1.9419  0.0001  87.9143\n","     76            \u001b[36m0.5344\u001b[0m        \u001b[32m0.9984\u001b[0m            0.2188        1.9457  0.0001  87.3629\n","     77            \u001b[36m0.5375\u001b[0m        0.9985            0.2062        1.9555  0.0001  87.7925\n","     78            0.5352        \u001b[32m0.9898\u001b[0m            0.2062        1.9612  0.0001  87.9653\n","     79            0.5297        \u001b[32m0.9791\u001b[0m            0.2000        1.9682  0.0001  87.3892\n","     80            0.5359        0.9895            0.2062        1.9863  0.0001  88.1118\n","     81            \u001b[36m0.5461\u001b[0m        \u001b[32m0.9754\u001b[0m            0.2125        1.9842  0.0001  88.1679\n","     82            0.5250        0.9887            0.2062        1.9808  0.0001  88.1137\n","     83            0.5312        0.9837            0.2188        1.9752  0.0001  87.6480\n","     84            0.5359        \u001b[32m0.9654\u001b[0m            0.2125        1.9993  0.0001  95.2982\n","     85            \u001b[36m0.5484\u001b[0m        \u001b[32m0.9572\u001b[0m            0.2188        1.9930  0.0001  105.3988\n","     86            0.5437        \u001b[32m0.9486\u001b[0m            0.2125        1.9994  0.0001  91.7300\n","     87            0.5477        \u001b[32m0.9449\u001b[0m            0.1938        2.0038  0.0001  109.1806\n","     88            \u001b[36m0.5617\u001b[0m        \u001b[32m0.9412\u001b[0m            0.2062        1.9834  0.0001  104.0104\n","     89            0.5555        \u001b[32m0.9282\u001b[0m            0.2000        1.9932  0.0001  107.4905\n","     90            0.5523        0.9307            0.2375        1.9891  0.0001  99.3567\n","     91            \u001b[36m0.5672\u001b[0m        \u001b[32m0.9280\u001b[0m            0.2250        1.9848  0.0001  86.1730\n","     92            \u001b[36m0.5703\u001b[0m        \u001b[32m0.9212\u001b[0m            0.2062        2.0037  0.0001  118.9010\n","     93            \u001b[36m0.5758\u001b[0m        \u001b[32m0.9136\u001b[0m            0.2188        2.0014  0.0001  87.4142\n","     94            \u001b[36m0.5766\u001b[0m        0.9139            0.2125        1.9958  0.0001  87.7124\n","     95            0.5703        \u001b[32m0.9131\u001b[0m            0.2125        2.0138  0.0001  86.7389\n","     96            0.5711        \u001b[32m0.8995\u001b[0m            0.2125        2.0131  0.0001  86.3682\n","     97            0.5727        0.9114            0.2062        2.0080  0.0001  86.4669\n","     98            0.5758        0.9059            0.2000        2.0320  0.0001  86.0766\n","     99            0.5687        \u001b[32m0.8956\u001b[0m            0.2062        2.0302  0.0001  86.1696\n","    100            \u001b[36m0.5797\u001b[0m        \u001b[32m0.8821\u001b[0m            0.1750        2.0693  0.0001  86.6055\n","    101            0.5781        0.8849            0.2125        2.0248  0.0000  86.9021\n","    102            0.5789        0.8826            0.2000        2.0460  0.0000  88.1208\n","    103            \u001b[36m0.5805\u001b[0m        0.8827            0.2000        2.0334  0.0000  133.0188\n","    104            \u001b[36m0.5836\u001b[0m        \u001b[32m0.8807\u001b[0m            0.2000        2.0250  0.0000  87.2762\n","    105            \u001b[36m0.5844\u001b[0m        \u001b[32m0.8726\u001b[0m            0.1875        2.0607  0.0000  86.9844\n","    106            0.5836        \u001b[32m0.8651\u001b[0m            0.2000        2.0639  0.0000  87.0474\n","    107            0.5828        \u001b[32m0.8597\u001b[0m            0.2125        2.0644  0.0000  86.4519\n","    108            \u001b[36m0.5898\u001b[0m        0.8776            0.2062        2.0538  0.0000  86.3539\n","    109            \u001b[36m0.5961\u001b[0m        0.8605            0.2000        2.0688  0.0000  86.6633\n","    110            0.5711        0.8667            0.1688        2.0901  0.0000  86.9508\n","    111            \u001b[36m0.6047\u001b[0m        0.8667            0.2125        2.0731  0.0000  87.0162\n","    112            0.5875        0.8622            0.1938        2.0989  0.0000  86.3565\n","    113            0.5930        \u001b[32m0.8544\u001b[0m            0.2062        2.0722  0.0000  86.9999\n","    114            0.5906        \u001b[32m0.8466\u001b[0m            0.2188        2.0701  0.0000  89.2163\n","    115            0.5992        \u001b[32m0.8421\u001b[0m            0.2125        2.0538  0.0000  86.8498\n","    116            0.5945        \u001b[32m0.8368\u001b[0m            0.2000        2.0822  0.0000  113.8002\n","    117            0.6008        0.8533            0.2000        2.0858  0.0000  87.0703\n","    118            0.5992        0.8381            0.2062        2.0986  0.0000  86.8588\n","    119            0.5945        0.8368            0.2313        2.0743  0.0000  86.9326\n","    120            0.6039        \u001b[32m0.8339\u001b[0m            0.2125        2.0966  0.0000  86.9088\n","    121            0.6016        \u001b[32m0.8294\u001b[0m            0.2250        2.1052  0.0000  88.7103\n","    122            \u001b[36m0.6078\u001b[0m        0.8399            0.2062        2.1019  0.0000  87.9833\n","    123            0.6016        0.8314            0.2062        2.1282  0.0000  87.4363\n","    124            \u001b[36m0.6109\u001b[0m        \u001b[32m0.8214\u001b[0m            0.2000        2.1051  0.0000  88.1786\n","    125            0.6055        \u001b[32m0.8192\u001b[0m            0.2000        2.1133  0.0000  88.0676\n","    126            0.6094        0.8210            0.2125        2.0937  0.0000  87.5863\n","    127            0.6062        0.8242            0.2062        2.1459  0.0000  88.8759\n","    128            0.6070        \u001b[32m0.8189\u001b[0m            0.1938        2.1338  0.0000  88.3352\n","    129            0.6094        \u001b[32m0.8144\u001b[0m            0.1875        2.1174  0.0000  91.5458\n","    130            \u001b[36m0.6117\u001b[0m        \u001b[32m0.8137\u001b[0m            0.2000        2.1085  0.0000  86.6828\n","    131            \u001b[36m0.6141\u001b[0m        0.8163            0.1938        2.1131  0.0000  88.1493\n","    132            0.6039        0.8185            0.2125        2.1316  0.0000  88.4186\n","    133            0.6078        0.8294            0.2000        2.1171  0.0000  98.3111\n","    134            0.6102        \u001b[32m0.8126\u001b[0m            0.2000        2.1153  0.0000  91.9511\n","    135            0.6141        0.8144            0.2062        2.1275  0.0000  87.2546\n","    136            0.6102        \u001b[32m0.8105\u001b[0m            0.2062        2.0877  0.0000  86.9233\n","    137            0.6109        \u001b[32m0.8023\u001b[0m            0.2125        2.0961  0.0000  87.6994\n","    138            0.6141        0.8139            0.2250        2.1212  0.0000  88.1644\n","    139            0.6078        0.8070            0.2250        2.1309  0.0000  87.2557\n","    140            0.6094        0.8029            0.2000        2.1165  0.0000  89.3393\n","    141            0.6086        \u001b[32m0.7972\u001b[0m            0.2125        2.1133  0.0000  86.6276\n","    142            0.6078        0.8000            0.2125        2.1151  0.0000  90.7144\n","    143            \u001b[36m0.6164\u001b[0m        0.8005            0.2125        2.1407  0.0000  86.8699\n","    144            0.6094        0.8143            0.2125        2.1540  0.0000  93.3060\n","    145            0.6055        \u001b[32m0.7921\u001b[0m            0.2125        2.1420  0.0000  87.9255\n","    146            0.6141        0.7996            0.2313        2.0861  0.0000  88.1231\n","    147            \u001b[36m0.6180\u001b[0m        0.7961            0.2188        2.1040  0.0000  88.3876\n","    148            0.6125        \u001b[32m0.7871\u001b[0m            0.2125        2.1104  0.0000  87.3470\n","    149            0.6180        \u001b[32m0.7860\u001b[0m            0.2188        2.1041  0.0000  87.8103\n","    150            0.6148        0.7923            0.2062        2.1282  0.0000  87.6511\n","    151            \u001b[36m0.6203\u001b[0m        \u001b[32m0.7853\u001b[0m            0.2000        2.1464  0.0000  87.6011\n","    152            \u001b[36m0.6227\u001b[0m        0.7909            0.2125        2.1266  0.0000  88.1779\n","    153            0.6195        \u001b[32m0.7760\u001b[0m            0.2125        2.1507  0.0000  133.0197\n","    154            0.6188        0.7900            0.2188        2.1009  0.0000  87.9698\n","    155            0.6203        0.7843            0.2062        2.1150  0.0000  88.8391\n","    156            0.6180        0.7845            0.2188        2.1281  0.0000  99.3387\n","    157            0.6141        0.7822            0.2188        2.1402  0.0000  98.3168\n","    158            0.6180        \u001b[32m0.7744\u001b[0m            0.2188        2.1346  0.0000  98.3970\n","    159            0.6203        0.7857            0.2125        2.1317  0.0000  97.7905\n","    160            \u001b[36m0.6234\u001b[0m        0.7809            0.2250        2.1591  0.0000  98.8089\n","    161            0.6172        0.7870            0.2188        2.1795  0.0000  97.8583\n","    162            0.6234        0.7842            0.2188        2.1634  0.0000  96.7899\n","    163            0.6219        0.7835            0.2125        2.1348  0.0000  130.5504\n","    164            0.6195        0.7857            0.2125        2.1382  0.0000  97.1375\n","    165            0.6211        0.7903            0.2188        2.1435  0.0000  103.5162\n","    166            0.6195        0.7817            0.2250        2.1349  0.0000  98.1968\n","    167            0.6195        0.7840            0.2250        2.1389  0.0000  97.0931\n","    168            0.6227        0.7789            0.2188        2.1384  0.0000  100.5972\n","    169            0.6234        0.7794            0.2250        2.1366  0.0000  99.6933\n","    170            \u001b[36m0.6242\u001b[0m        0.7796            0.2125        2.1438  0.0000  100.4953\n","    171            0.6234        \u001b[32m0.7742\u001b[0m            0.2188        2.1395  0.0000  98.7813\n","    172            \u001b[36m0.6266\u001b[0m        0.7745            0.2188        2.1434  0.0000  94.8950\n","    173            0.6258        0.7805            0.2125        2.1496  0.0000  102.4587\n","    174            \u001b[36m0.6273\u001b[0m        0.7779            0.2125        2.1498  0.0000  101.0185\n","    175            \u001b[36m0.6297\u001b[0m        \u001b[32m0.7740\u001b[0m            0.2125        2.1470  0.0000  98.7133\n","    176            0.6266        0.7776            0.2125        2.1512  0.0000  100.1620\n","    177            0.6227        0.7879            0.2250        2.1452  0.0000  95.7315\n","    178            0.6211        0.7796            0.2125        2.1475  0.0000  100.0145\n","    179            0.6242        0.7749            0.2188        2.1447  0.0000  105.2348\n","    180            0.6250        \u001b[32m0.7692\u001b[0m            0.2188        2.1446  0.0000  116.9271\n","    181            0.6234        0.7837            0.2250        2.1429  0.0000  100.6156\n","    182            0.6250        0.7816            0.2250        2.1451  0.0000  97.7623\n","    183            0.6242        0.7893            0.2188        2.1426  0.0000  97.8503\n","    184            0.6234        0.7714            0.2250        2.1455  0.0000  102.5719\n","    185            0.6227        0.7843            0.2188        2.1449  0.0000  100.2905\n","    186            0.6234        0.7737            0.2250        2.1449  0.0000  102.7047\n","    187            0.6234        0.7725            0.2188        2.1455  0.0000  102.3615\n","    188            0.6227        0.7775            0.2250        2.1477  0.0000  106.8036\n","    189            0.6227        0.7829            0.2250        2.1499  0.0000  102.1793\n","    190            0.6242        0.7753            0.2250        2.1518  0.0000  105.1639\n","    191            0.6250        0.7747            0.2250        2.1519  0.0000  104.4638\n","    192            0.6242        0.7782            0.2250        2.1510  0.0000  107.9270\n","    193            0.6242        0.7753            0.2250        2.1506  0.0000  104.2435\n","    194            0.6242        0.7882            0.2250        2.1515  0.0000  100.1793\n","    195            0.6242        0.7708            0.2250        2.1520  0.0000  98.9245\n","    196            0.6242        0.7736            0.2250        2.1523  0.0000  100.0120\n","    197            0.6242        0.7787            0.2250        2.1525  0.0000  99.8559\n","    198            0.6242        0.7821            0.2250        2.1526  0.0000  101.3262\n","    199            0.6242        0.7789            0.2250        2.1526  0.0000  101.7029\n","    200            0.6242        0.7843            0.2250        2.1526  0.0000  101.2538\n"]},{"data":{"text/plain":["<class 'braindecode.classifier.EEGClassifier'>[initialized](\n","  module_=TSception(\n","    (Tception): Sequential(\n","      (0): Conv2d(1, 64, kernel_size=(1, 101), stride=(1, 1), padding=(0, 50))\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU()\n","    )\n","    (Sception): Sequential(\n","      (0): Conv2d(64, 64, kernel_size=(208, 1), stride=(1, 1))\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU()\n","    )\n","    (lstm): LSTM(60, 60, batch_first=True, dropout=0.5)\n","    (linear): Linear(in_features=60, out_features=5, bias=True)\n","  ),\n",")"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}]}]}