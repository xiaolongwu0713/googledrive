{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"main_base.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1oNSddmPYT8S4F9Ngr6nwU3pTPIoHVSQs","authorship_tag":"ABX9TyNGht0x+nsri3CZeHvitEde"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CYz-P2Bj7TIm"},"source":["# 1.   results:\n","\n","## windows=500\n","*   rawAndbands input: (windows/stride/accuracy)=500/50/99, 500/100/91, 500/200/77\n","*   raw input: (windows/stride/accuracy)= 500/50/99, 500/100/97, 500/200/83\n","\n","## windows=200\n","*  raw input: 200/200/44, 200/100/74, 200/50/96, \n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ANOR3DiaMpoE","executionInfo":{"status":"ok","timestamp":1633225337485,"user_tz":-480,"elapsed":300,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}},"outputId":"ced62d2e-12c1-4541-ff91-cfbe2d8ad757"},"source":["%cd /content/drive/MyDrive/"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]}]},{"cell_type":"code","metadata":{"id":"75yR8ulWMu62","executionInfo":{"status":"ok","timestamp":1633225410000,"user_tz":-480,"elapsed":71197,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["%%capture\n","! pip install hdf5storage\n","! pip install mne==0.23.0\n","! pip install torch==1.7.0\n","! pip install Braindecode==0.5.1\n","! pip install timm"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"YAiYT0gbMyaX","executionInfo":{"status":"ok","timestamp":1633225411941,"user_tz":-480,"elapsed":1956,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["import scipy.io\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import mne\n","import torch\n","from torch.optim import lr_scheduler\n","from torch import nn\n","import timm\n","from common_dl import myDataset\n","from torch.utils.data import DataLoader\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from common_dl import set_random_seeds\n","from common_dsp import *\n","from gesture.models.d2l_resnet import d2lresnet\n","from myskorch import on_epoch_begin_callback, on_batch_end_callback\n","from ecog_finger.config import *\n","from ecog_finger.preprocess.chn_settings import  get_channel_setting\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"vkgUv_2zM0_9","executionInfo":{"status":"ok","timestamp":1633225411942,"user_tz":-480,"elapsed":7,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["try:\n","    mne.set_config('MNE_LOGGING_LEVEL','ERROR')\n","except TypeError as err:\n","    print(err)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"-NBuF2-rM2Os","executionInfo":{"status":"ok","timestamp":1633225411943,"user_tz":-480,"elapsed":8,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["sid=2\n","fs=1000\n","use_active_only=False\n","if use_active_only:\n","    active_chn=get_channel_setting(sid)\n","else:\n","    active_chn='all'"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"YYAJF-LRVYlO","executionInfo":{"status":"ok","timestamp":1633225423807,"user_tz":-480,"elapsed":3088,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["project_dir=data_dir+'fingerflex/data/'+str(sid)+'/'\n","model_path=project_dir + 'pth' +'/'\n","input='rawAndbands'\n","input='raw'\n","if input=='raw':\n","    filename=project_dir + str(sid)+'_fingerflex.mat'\n","    mat=scipy.io.loadmat(filename)\n","    data=mat['data'] # (46, 610040)\n","    data=data[:,:-1]\n","\n","    if 1==1:\n","        scaler = StandardScaler()\n","        scaler.fit(data)\n","        data=scaler.transform((data))\n","    data=np.transpose(data)\n","    chn_num=data.shape[0]\n","    flex=np.transpose(mat['flex']) #(5, 610040)\n","    cue=np.transpose(mat['cue']) # (1, 610040)\n","    data=np.concatenate((data,cue),axis=0) # (47, 610040) / (47, 610040)\n","\n","    chn_names=np.append([\"ecog\"]*chn_num,[\"stim\"])  #,\"thumb\",\"index\",\"middle\",\"ring\",\"little\"])\n","    chn_types=np.append([\"ecog\"]*chn_num,[\"stim\"])  #, \"emg\",\"emg\",\"emg\",\"emg\",\"emg\"])\n","    info = mne.create_info(ch_names=list(chn_names), ch_types=list(chn_types), sfreq=fs)\n","    raw = mne.io.RawArray(data, info)\n","\n","    events = mne.find_events(raw, stim_channel='stim')\n","    events=events-[0,0,1] #(150, 3)\n","    raw=raw.pick(picks=['ecog'])\n","\n","\n","    epochs = mne.Epochs(raw, events, tmin=0, tmax=2,baseline=None)\n","    # or epoch from 0s to 4s which only contain movement data.\n","    # epochs = mne.Epochs(raw, events1, tmin=0, tmax=4,baseline=None)\n","\n","    epoch1=epochs['0'].get_data() # 20 trials. 8001 time points per trial for 8s.\n","    epoch2=epochs['1'].get_data()\n","    epoch3=epochs['2'].get_data()\n","    epoch4=epochs['3'].get_data()\n","    epoch5=epochs['4'].get_data()\n","    list_of_epochs = [epoch1, epoch2, epoch3, epoch4, epoch5]\n","    chn_num=epoch1.shape[1]\n","\n","elif input=='rawAndbands':\n","    list_of_epochs=[]\n","    save_to = data_dir + 'fingerflex/data/' + str(sid) + '/'\n","    for fingeri in range(5):\n","        tmp = mne.read_epochs(save_to + 'rawBandEpoch'+str(fingeri)+'.fif')\n","        list_of_epochs.append(tmp.get_data())\n","    chn_num=list_of_epochs[0].shape[1]\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"v5-_7mGqNGgm","executionInfo":{"status":"ok","timestamp":1633225524265,"user_tz":-480,"elapsed":294,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["wind=200\n","stride=50\n","s=0\n","X_train=[]\n","X_test=[]\n","labels_train=[]\n","labels_test=[]\n","total_len=list_of_epochs[0].shape[2]\n","labels=[]"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvNXKIIpNJMp","executionInfo":{"status":"ok","timestamp":1633225530415,"user_tz":-480,"elapsed":314,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["test_number=10\n","for i in range(5):\n","    Xi_train = []\n","    Xi_test = []\n","    Xi=[]\n","    #ss=(total_len-wind)//stride\n","    for trial in list_of_epochs[i]: # (63, 2001)\n","        #trial=np.concatenate((trial,trial[-3:,:]),axis=0)\n","        s = 0\n","        while stride*s+wind<total_len:\n","            start=s * stride\n","            tmp=trial[:,start:(start+wind)]\n","            Xi.append(tmp)\n","            s=s+1\n","        # add the last window\n","        last_s=s-1\n","        if stride * last_s + wind<total_len-100:\n","            tmp=trial[:,-wind:]\n","            Xi.append(tmp)\n","    Xi_train=np.asarray(Xi[:-test_number]) # (260, 63, 500)\n","    Xi_test = np.asarray(Xi[-test_number:]) # (10, 63, 500)\n","\n","    X_train.append(Xi_train)\n","    X_test.append(Xi_test)\n","\n","    samples_number=len(Xi)\n","    label=[i]*samples_number\n","    label_train=label[:-test_number]\n","    label_test=label[-test_number:]\n","\n","    labels_train.append(label_train)\n","    labels_test.append(label_test)\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Vn1oB8EdwN4","executionInfo":{"status":"ok","timestamp":1633225533483,"user_tz":-480,"elapsed":517,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["X_train=np.concatenate(X_train,axis=0) # (1300, 63, 500)\n","X_test=np.concatenate(X_test,axis=0) # (50, 63, 500)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"WelESGUdNN2q","executionInfo":{"status":"ok","timestamp":1633225536179,"user_tz":-480,"elapsed":500,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["labels_train=np.asarray(labels_train)\n","labels_train=np.reshape(labels_train,(-1,1)) # (5, 270)\n","labels_test=np.asarray(labels_test)\n","labels_test=np.reshape(labels_test,(-1,1)) # (5, 270)\n","\n","# (871, 63, 500)/ (429, 63, 500)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, labels_train, test_size=0.33, random_state=42)\n","\n","train_set=myDataset(X_train,y_train)\n","val_set=myDataset(X_val,y_val)\n","\n","batch_size = 32\n","train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, pin_memory=False)\n","val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=True, pin_memory=False)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"vZsxxcEaj6sy","executionInfo":{"status":"ok","timestamp":1633225536180,"user_tz":-480,"elapsed":6,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["train_size=len(train_loader.dataset)\n","val_size=len(val_loader.dataset)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"fwqm_nOlNQKd","executionInfo":{"status":"ok","timestamp":1633225537433,"user_tz":-480,"elapsed":3,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n","device = 'cuda' if cuda else 'cpu'\n","if cuda:\n","    torch.backends.cudnn.benchmark = True\n","seed = 20200220  # random seed to make results reproducible\n","set_random_seeds(seed=seed)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wxs7p8ikNSOX","executionInfo":{"status":"ok","timestamp":1633225540767,"user_tz":-480,"elapsed":3042,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["#net=d2lresnet()\n","img_size=[chn_num,wind]\n","net = timm.create_model('visformer_tiny',num_classes=5,in_chans=1,img_size=img_size)\n","net = net.to(device)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"2fCL4EeV4jYa"},"source":["load_epoch=\n","net = timm.create_model('visformer_tiny',num_classes=5,in_chans=1,img_size=img_size)\n","net = net.to(device)\n","optimizer = torch.optim.Adadelta(net.parameters(), lr=lr)\n","checkpoint=torch.load()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-VY-5J7PbIV_","executionInfo":{"status":"ok","timestamp":1633225540768,"user_tz":-480,"elapsed":7,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":["lr = 0.05\n","weight_decay = 1e-10\n","epoch_num = 500\n","\n","criterion = nn.CrossEntropyLoss()\n","#criterion = nn.NLLLoss()\n","#optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n","optimizer = torch.optim.Adadelta(net.parameters(), lr=lr)\n","#optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","# Decay LR by a factor of 0.1 every 7 epochs\n","lr_schedulerr = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n","epoch_num = 500"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"UFwHkb2dNXER","executionInfo":{"status":"error","timestamp":1633226382566,"user_tz":-480,"elapsed":833442,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}},"outputId":"57981f38-2041-460f-deda-4b292eab08a2"},"source":["for epoch in range(epoch_num):\n","    print(\"------ epoch \" + str(epoch) + \" -----\")\n","    net.train()\n","\n","    loss_epoch = 0\n","\n","    running_loss = 0.0\n","    running_corrects = 0\n","    for batch, (trainx, trainy) in enumerate(train_loader):\n","        trainx=torch.unsqueeze(trainx,dim=1)\n","        optimizer.zero_grad()\n","        if (cuda):\n","            trainx = trainx.float().cuda()\n","        else:\n","            trainx = trainx.float()\n","        y_pred = net(trainx)\n","        #print(\"y_pred shape: \" + str(y_pred.shape))\n","        preds = y_pred.argmax(dim=1, keepdim=True)\n","        #_, preds = torch.max(y_pred, 1)\n","\n","        if cuda:\n","            loss = criterion(y_pred, trainy.squeeze().cuda())\n","        else:\n","            loss = criterion(y_pred, trainy.squeeze())\n","\n","        loss.backward()  # calculate the gradient and store in .grad attribute.\n","        optimizer.step()\n","        running_loss += loss.item() * trainx.shape[0]\n","        running_corrects += torch.sum(preds.cpu().squeeze() == trainy.squeeze())\n","    #print(\"train_size: \" + str(train_size))\n","    lr_schedulerr.step() # test it\n","    epoch_loss = running_loss / train_size\n","    epoch_acc = running_corrects.double() / train_size\n","    print(\"Training loss: {:.2f}; Accuracy: {:.2f}.\".format(epoch_loss,epoch_acc.item()))\n","    #print(\"Training \" + str(epoch) + \": loss: \" + str(epoch_loss) + \",\" + \"Accuracy: \" + str(epoch_acc.item()) + \".\")\n","\n","    state = {\n","            'net': net.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","        }\n","    savepath = model_path + 'checkpoint' + str(epoch) + '.pth'\n","    torch.save(state, savepath)\n","    running_loss = 0.0\n","    running_corrects = 0\n","    if epoch % 1 == 0:\n","        net.eval()\n","        # print(\"Validating...\")\n","        with torch.no_grad():\n","            for _, (val_x, val_y) in enumerate(val_loader):\n","                val_x = torch.unsqueeze(val_x, dim=1)\n","                if (cuda):\n","                    val_x = val_x.float().cuda()\n","                    # val_y = val_y.float().cuda()\n","                else:\n","                    val_x = val_x.float()\n","                    # val_y = val_y.float()\n","                outputs = net(val_x)\n","                #_, preds = torch.max(outputs, 1)\n","                preds = outputs.argmax(dim=1, keepdim=True)\n","\n","                running_corrects += torch.sum(preds.cpu().squeeze() == val_y.squeeze())\n","\n","        epoch_acc = running_corrects.double() / val_size\n","        print(\"Evaluation accuracy: {:.2f}.\".format(epoch_acc.item()))\n"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["------ epoch 0 -----\n","Training loss: 1.34; Accuracy: 0.44.\n","Evaluation accuracy: 0.51.\n","------ epoch 1 -----\n","Training loss: 0.90; Accuracy: 0.66.\n","Evaluation accuracy: 0.61.\n","------ epoch 2 -----\n","Training loss: 0.58; Accuracy: 0.79.\n","Evaluation accuracy: 0.70.\n","------ epoch 3 -----\n","Training loss: 0.36; Accuracy: 0.88.\n","Evaluation accuracy: 0.76.\n","------ epoch 4 -----\n","Training loss: 0.23; Accuracy: 0.92.\n","Evaluation accuracy: 0.85.\n","------ epoch 5 -----\n","Training loss: 0.17; Accuracy: 0.94.\n","Evaluation accuracy: 0.80.\n","------ epoch 6 -----\n","Training loss: 0.12; Accuracy: 0.96.\n","Evaluation accuracy: 0.79.\n","------ epoch 7 -----\n","Training loss: 0.08; Accuracy: 0.98.\n","Evaluation accuracy: 0.87.\n","------ epoch 8 -----\n","Training loss: 0.06; Accuracy: 0.98.\n","Evaluation accuracy: 0.89.\n","------ epoch 9 -----\n","Training loss: 0.05; Accuracy: 0.99.\n","Evaluation accuracy: 0.89.\n","------ epoch 10 -----\n","Training loss: 0.05; Accuracy: 0.99.\n","Evaluation accuracy: 0.87.\n","------ epoch 11 -----\n","Training loss: 0.05; Accuracy: 0.98.\n","Evaluation accuracy: 0.90.\n","------ epoch 12 -----\n","Training loss: 0.03; Accuracy: 0.99.\n","Evaluation accuracy: 0.91.\n","------ epoch 13 -----\n","Training loss: 0.02; Accuracy: 0.99.\n","Evaluation accuracy: 0.87.\n","------ epoch 14 -----\n","Training loss: 0.04; Accuracy: 0.99.\n","Evaluation accuracy: 0.85.\n","------ epoch 15 -----\n","Training loss: 0.03; Accuracy: 0.99.\n","Evaluation accuracy: 0.89.\n","------ epoch 16 -----\n","Training loss: 0.02; Accuracy: 1.00.\n","Evaluation accuracy: 0.89.\n","------ epoch 17 -----\n","Training loss: 0.02; Accuracy: 0.99.\n","Evaluation accuracy: 0.86.\n","------ epoch 18 -----\n","Training loss: 0.03; Accuracy: 0.99.\n","Evaluation accuracy: 0.93.\n","------ epoch 19 -----\n","Training loss: 0.01; Accuracy: 1.00.\n","Evaluation accuracy: 0.92.\n","------ epoch 20 -----\n","Training loss: 0.01; Accuracy: 1.00.\n","Evaluation accuracy: 0.94.\n","------ epoch 21 -----\n","Training loss: 0.01; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 22 -----\n","Training loss: 0.01; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 23 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 24 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 25 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 26 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 27 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 28 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 29 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 30 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 31 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 32 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 33 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 34 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 35 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 36 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 37 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 38 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 39 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 40 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 41 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 42 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 43 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 44 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 45 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 46 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 47 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 48 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 49 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 50 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 51 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 52 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 53 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 54 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 55 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 56 -----\n","Training loss: 0.01; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 57 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 58 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 59 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 60 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 61 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 62 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 63 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 64 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 65 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 66 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 67 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 68 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 69 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 70 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 71 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 72 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 73 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 74 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 75 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 76 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 77 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 78 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 79 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 80 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 81 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 82 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 83 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 84 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 85 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 86 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 87 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 88 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 89 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 90 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 91 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 92 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 93 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 94 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 95 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 96 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 97 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 98 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 99 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 100 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 101 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 102 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 103 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 104 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 105 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 106 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 107 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 108 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.95.\n","------ epoch 109 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 110 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 111 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 112 -----\n","Training loss: 0.00; Accuracy: 1.00.\n","Evaluation accuracy: 0.96.\n","------ epoch 113 -----\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-d4b0ab3fada0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mtrainx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m#print(\"y_pred shape: \" + str(y_pred.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/timm/models/visformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/timm/models/visformer.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;31m# stage 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/timm/models/visformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/timm/models/visformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"Nv-zbCVueV75","executionInfo":{"status":"aborted","timestamp":1633224933867,"user_tz":-480,"elapsed":6,"user":{"displayName":"Long WU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYKa6__vAaw1RpvqUkOWX7cZ5xvDbUQ1ovLHLL=s64","userId":"09414210733761439327"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dj227FHByFPh"},"source":["if epoch % 1 == 0:\n","        net.eval()\n","        # print(\"Validating...\")\n","        with torch.no_grad():\n","            for _, (val_x, val_y) in enumerate(val_loader):\n","                val_x = torch.unsqueeze(val_x, dim=1)\n","                if (cuda):\n","                    val_x = val_x.float().cuda()\n","                    # val_y = val_y.float().cuda()\n","                else:\n","                    val_x = val_x.float()\n","                    # val_y = val_y.float()\n","                outputs = net(val_x)\n","                #_, preds = torch.max(outputs, 1)\n","                preds = outputs.argmax(dim=1, keepdim=True)\n","\n","                running_corrects += torch.sum(preds.cpu().squeeze() == val_y.squeeze())\n","\n","        epoch_acc = running_corrects.double() / val_size\n","        print(\"Evaluation accuracy: {:.2f}.\".format(epoch_acc.item()))\n"],"execution_count":null,"outputs":[]}]}